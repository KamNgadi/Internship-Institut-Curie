{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "Setup complete âœ… (72 CPUs, 250.5 GB RAM, 421.4/1876.2 GB disk)\n",
      "\n",
      "OS                  Linux-5.15.0-113-generic-x86_64-with-glibc2.29\n",
      "Environment         Linux\n",
      "Python              3.8.10\n",
      "Install             git\n",
      "RAM                 250.54 GB\n",
      "CPU                 Intel Xeon Gold 5220 2.20GHz\n",
      "CUDA                12.1\n",
      "\n",
      "numpy               âœ… 1.24.4<2.0.0,>=1.23.0\n",
      "matplotlib          âœ… 3.7.5>=3.3.0\n",
      "opencv-python       âœ… 4.8.0.74>=4.6.0\n",
      "pillow              âœ… 10.2.0>=7.1.2\n",
      "pyyaml              âœ… 6.0.1>=5.3.1\n",
      "requests            âœ… 2.31.0>=2.23.0\n",
      "scipy               âœ… 1.10.1>=1.4.1\n",
      "torch               âœ… 2.2.1>=1.8.0\n",
      "torchvision         âœ… 0.17.1>=0.9.0\n",
      "tqdm                âœ… 4.66.2>=4.64.0\n",
      "psutil              âœ… 5.9.8\n",
      "py-cpuinfo          âœ… 9.0.0\n",
      "pandas              âœ… 2.0.3>=1.1.4\n",
      "seaborn             âœ… 0.13.2>=0.11.0\n",
      "ultralytics-thop    âœ… 2.0.0>=2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import shutil\n",
    "from operator import is_not\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "!yolo checks\n",
    "HOME = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version              Location                                                   \n",
      "------------------------ -------------------- -----------------------------------------------------------\n",
      "absl-py                  2.1.0                \n",
      "aiosignal                1.3.1                \n",
      "apturl                   0.5.2                \n",
      "attrs                    19.3.0               \n",
      "backcall                 0.1.0                \n",
      "bcrypt                   3.1.7                \n",
      "bleach                   3.1.1                \n",
      "blinker                  1.4                  \n",
      "Brlapi                   0.7.0                \n",
      "cachetools               5.4.0                \n",
      "certifi                  2023.7.22            \n",
      "chardet                  4.0.0                \n",
      "charset-normalizer       3.3.2                \n",
      "Click                    7.0                  \n",
      "colorama                 0.4.3                \n",
      "command-not-found        0.3                  \n",
      "contourpy                1.1.1                \n",
      "cryptography             2.8                  \n",
      "cupshelpers              1.0                  \n",
      "cycler                   0.10.0               \n",
      "dbus-python              1.2.16               \n",
      "decorator                4.4.2                \n",
      "defer                    1.0.6                \n",
      "defusedxml               0.6.0                \n",
      "dill                     0.3.8                \n",
      "distro                   1.4.0                \n",
      "distro-info              0.23+ubuntu1.1       \n",
      "dnspython                1.16.0               \n",
      "duplicity                0.8.12.0             \n",
      "entrypoints              0.3                  \n",
      "fasteners                0.14.1               \n",
      "filelock                 3.13.1               \n",
      "fonttools                4.49.0               \n",
      "frozenlist               1.4.1                \n",
      "fsspec                   2024.2.0             \n",
      "future                   0.18.2               \n",
      "google-auth              2.32.0               \n",
      "google-auth-oauthlib     1.0.0                \n",
      "gpg                      1.13.1               \n",
      "grpcio                   1.65.2               \n",
      "html5lib                 1.0.1                \n",
      "httplib2                 0.14.0               \n",
      "idna                     2.10                 \n",
      "imageio                  2.34.2               \n",
      "importlib-metadata       1.5.0                \n",
      "importlib-resources      6.1.3                \n",
      "ipykernel                5.2.0                \n",
      "ipython                  7.13.0               \n",
      "ipython-genutils         0.2.0                \n",
      "ipywidgets               6.0.0                \n",
      "jedi                     0.15.2               \n",
      "Jinja2                   2.10.1               \n",
      "joblib                   1.3.2                \n",
      "jsonschema               3.2.0                \n",
      "jupyter-client           6.1.2                \n",
      "jupyter-console          6.0.0                \n",
      "jupyter-core             4.6.3                \n",
      "keyring                  18.0.1               \n",
      "kiwisolver               1.4.5                \n",
      "language-selector        0.1                  \n",
      "launchpadlib             1.10.13              \n",
      "lazr.restfulclient       0.14.2               \n",
      "lazr.uri                 1.0.3                \n",
      "lazy-loader              0.4                  \n",
      "lockfile                 0.12.2               \n",
      "louis                    3.12.0               \n",
      "macaroonbakery           1.3.1                \n",
      "Mako                     1.1.0                \n",
      "Markdown                 3.1.1                \n",
      "MarkupSafe               2.1.5                \n",
      "matplotlib               3.7.5                \n",
      "mistune                  0.8.4                \n",
      "monotonic                1.5                  \n",
      "more-itertools           4.2.0                \n",
      "mpmath                   1.3.0                \n",
      "msgpack                  1.0.8                \n",
      "nbconvert                5.6.1                \n",
      "nbformat                 5.0.4                \n",
      "netifaces                0.10.4               \n",
      "networkx                 3.1                  \n",
      "notebook                 6.0.3                \n",
      "numpy                    1.24.4               \n",
      "nvidia-cublas-cu12       12.1.3.1             \n",
      "nvidia-cuda-cupti-cu12   12.1.105             \n",
      "nvidia-cuda-nvrtc-cu12   12.1.105             \n",
      "nvidia-cuda-runtime-cu12 12.1.105             \n",
      "nvidia-cudnn-cu12        8.9.2.26             \n",
      "nvidia-cufft-cu12        11.0.2.54            \n",
      "nvidia-curand-cu12       10.3.2.106           \n",
      "nvidia-cusolver-cu12     11.4.5.107           \n",
      "nvidia-cusparse-cu12     12.1.0.106           \n",
      "nvidia-nccl-cu12         2.19.3               \n",
      "nvidia-nvjitlink-cu12    12.4.99              \n",
      "nvidia-nvtx-cu12         12.1.105             \n",
      "oauthlib                 3.1.0                \n",
      "olefile                  0.46                 \n",
      "opencv-python            4.8.0.74             \n",
      "packaging                24.1                 \n",
      "pandas                   2.0.3                \n",
      "pandocfilters            1.4.2                \n",
      "paramiko                 2.6.0                \n",
      "parso                    0.5.2                \n",
      "pexpect                  4.6.0                \n",
      "pickleshare              0.7.5                \n",
      "pillow                   10.2.0               \n",
      "pip                      20.0.2               \n",
      "prometheus-client        0.7.1                \n",
      "prompt-toolkit           2.0.10               \n",
      "protobuf                 5.26.1               \n",
      "psutil                   5.9.8                \n",
      "py-cpuinfo               9.0.0                \n",
      "pyarrow                  15.0.0               \n",
      "pyasn1                   0.6.0                \n",
      "pyasn1-modules           0.4.0                \n",
      "pycairo                  1.16.2               \n",
      "pycrypto                 2.6.1                \n",
      "pycups                   1.9.73               \n",
      "Pygments                 2.3.1                \n",
      "PyGObject                3.36.0               \n",
      "PyJWT                    1.7.1                \n",
      "pylance                  0.10.15              \n",
      "pymacaroons              0.13.0               \n",
      "PyNaCl                   1.3.0                \n",
      "pyparsing                2.4.6                \n",
      "pyRFC3339                1.1                  \n",
      "pyrsistent               0.15.5               \n",
      "python-apt               2.0.1+ubuntu0.20.4.1 \n",
      "python-dateutil          2.9.0.post0          \n",
      "python-debian            0.1.36+ubuntu1.1     \n",
      "python-dotenv            1.0.1                \n",
      "python-magic             0.4.27               \n",
      "pytz                     2024.1               \n",
      "PyWavelets               1.4.1                \n",
      "pyxdg                    0.26                 \n",
      "PyYAML                   6.0.1                \n",
      "pyzmq                    18.1.1               \n",
      "ray                      2.9.3                \n",
      "reportlab                3.5.34               \n",
      "requests                 2.31.0               \n",
      "requests-oauthlib        2.0.0                \n",
      "requests-toolbelt        1.0.0                \n",
      "requests-unixsocket      0.2.0                \n",
      "roboflow                 1.1.23               \n",
      "rsa                      4.9                  \n",
      "scikit-image             0.21.0               \n",
      "scikit-learn             1.3.2                \n",
      "scipy                    1.10.1               \n",
      "screen-resolution-extra  0.0.0                \n",
      "seaborn                  0.13.2               \n",
      "SecretStorage            2.3.1                \n",
      "Send2Trash               1.5.0                \n",
      "setuptools               45.2.0               \n",
      "simplejson               3.16.0               \n",
      "six                      1.14.0               \n",
      "ssh-import-id            5.10                 \n",
      "sympy                    1.12                 \n",
      "systemd-python           234                  \n",
      "tensorboard              2.14.0               \n",
      "tensorboard-data-server  0.7.2                \n",
      "tensorboardX             2.6.2.2              \n",
      "terminado                0.8.2                \n",
      "testpath                 0.4.4                \n",
      "thop                     0.1.1.post2209072238 \n",
      "threadpoolctl            3.3.0                \n",
      "tifffile                 2023.7.10            \n",
      "torch                    2.2.1                \n",
      "torchinfo                1.8.0                \n",
      "torchvision              0.17.1               \n",
      "tornado                  5.1.1                \n",
      "tqdm                     4.66.2               \n",
      "traitlets                4.3.3                \n",
      "triton                   2.2.0                \n",
      "typing-extensions        4.10.0               \n",
      "tzdata                   2024.1               \n",
      "ubuntu-drivers-common    0.0.0                \n",
      "ubuntu-pro-client        8001                 \n",
      "ufw                      0.36                 \n",
      "ultralytics              8.2.70               /home/kamenan/Documents/Stage Dev Deep Learning/ultralytics\n",
      "ultralytics-thop         2.0.0                \n",
      "unattended-upgrades      0.1                  \n",
      "urllib3                  2.2.1                \n",
      "usb-creator              0.3.7                \n",
      "wadllib                  1.3.3                \n",
      "wcwidth                  0.1.8                \n",
      "webencodings             0.5.1                \n",
      "werkzeug                 3.0.3                \n",
      "wheel                    0.34.2               \n",
      "widgetsnbextension       2.0.0                \n",
      "xkit                     0.0.0                \n",
      "zipp                     3.17.0               \n"
     ]
    }
   ],
   "source": [
    "#!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Exemple de tableau de forme (10, 256, 256, 3)\n",
    "Y = np.random.random((10, 256, 256, 3))\n",
    "\n",
    "# SÃ©lection de l'Ã©lÃ©ment\n",
    "i = 0\n",
    "result = Y[i, ..., 2]\n",
    "\n",
    "print(result.shape)  # (256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(x, axis=0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install opencv-python==4.8.0.74\n",
    "#?tune.uniform()\n",
    "HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_mask_pairs(data_dir):\n",
    "    \"\"\"\n",
    "    retrieve image and mask from a given directory\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    mask_paths = []\n",
    "    \n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        if 'images' in root:\n",
    "            for file in files:\n",
    "                if file.endswith('.png'):\n",
    "                    image_paths.append(os.path.join(root, file))\n",
    "                    #print(f'{root}\\n, {files}') \n",
    "                    mask_paths.append(os.path.join(root.replace('images', 'masks'), file.replace(\"img\", \"masks\")))\n",
    "                      \n",
    "    return image_paths, mask_paths\n",
    "\n",
    "def mask_to_polygons(mask):\n",
    "\n",
    "    \"\"\"\n",
    "        Draw contours arround the mask \n",
    "    \"\"\"\n",
    "\n",
    "    # Find the contour of an object precisely the coordonnates of the boundary shape\n",
    "    # Use RETR_TREE retrievial hierachy mode \n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        if len(contour) > 2:\n",
    "            poly = contour.reshape(-1).tolist()\n",
    "            if len(poly) > 4:  # Ensure valid polygon\n",
    "                polygons.append(poly)\n",
    "    return polygons\n",
    "\n",
    "def process_data(image_paths, mask_paths, output_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    images = []\n",
    "    image_id = 0\n",
    "    ann_id = 0\n",
    "    \n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        image_id += 1\n",
    "        img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED) #return the loaded image as is (with alpha channel, otherwise it gets cropped).\n",
    "        \n",
    "        # Copy image to output directory\n",
    "        shutil.copy(img_path, os.path.join(output_dir, os.path.basename(img_path)))\n",
    "        \n",
    "        images.append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": os.path.basename(img_path),\n",
    "            \"height\": img.shape[0],\n",
    "            \"width\": img.shape[1]\n",
    "        })\n",
    "        \n",
    "        unique_values = np.unique(mask)\n",
    "        for value in unique_values:\n",
    "            if value == 0:  # Ignore background\n",
    "                continue\n",
    "            \n",
    "            object_mask = (mask == value).astype(np.uint8) * 255\n",
    "            polygons = mask_to_polygons(object_mask)\n",
    "            \n",
    "            for poly in polygons:\n",
    "                ann_id += 1\n",
    "                annotations.append({\n",
    "                    \"id\": ann_id,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": 1,  # Only one category: Cellular\n",
    "                    \"segmentation\": [poly],\n",
    "                    \"area\": cv2.contourArea(np.array(poly).reshape(-1, 2)),\n",
    "                    \"bbox\": list(cv2.boundingRect(np.array(poly).reshape(-1, 2))),\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "    \n",
    "        coco_output = {\n",
    "            \"images\": images,\n",
    "            \"annotations\": annotations,\n",
    "            \"categories\": [{\"id\": 1, \"name\": \"Cellular\"}]\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'coco_annotations.json'), 'w') as f:\n",
    "            json.dump(coco_output, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = HOME\n",
    "output_dir = 'COCO_output'\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "val_dir = os.path.join(output_dir, 'val')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "image_paths, mask_paths = get_image_mask_pairs(data_dir)\n",
    "\n",
    "# Split data into train and val\n",
    "train_img_paths, val_img_paths, train_mask_paths, val_mask_paths = train_test_split(image_paths, mask_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "# Process train and val data\n",
    "process_data(train_img_paths, train_mask_paths, train_dir)\n",
    "process_data(val_img_paths, val_mask_paths, val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_with_coco_annotations(image_paths:str, annotations:str, display_type='both'):\n",
    "    \"\"\"\n",
    "    Lay out the images with the COCO annotations\n",
    "    Params:\n",
    "    ------\n",
    "         Image_paths: the paths where the converted COCO.json has been stored\n",
    "         annotations: path to anotations files\n",
    "         display_both: Lay out annotated images along side the original image\n",
    "\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    \n",
    "    for ax, img_path in zip(axs.ravel(), image_paths):\n",
    "        # Load image using OpenCV and convert it from BGR to RGB color space\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')  # Turn off the axes\n",
    "\n",
    "        # Get image filename to match with annotations\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        img_id = next(item for item in annotations['images'] if item[\"file_name\"] == img_filename)['id']\n",
    "        \n",
    "        # Filter annotations for the current image\n",
    "        img_annotations = [ann for ann in annotations['annotations'] if ann['image_id'] == img_id]\n",
    "        \n",
    "        # Generate random colors for each annotation\n",
    "        colors = [tuple(np.random.rand(3)) for _ in img_annotations]\n",
    "\n",
    "        for ann, color in zip(img_annotations, colors):\n",
    "            # Display bounding box\n",
    "            if display_type in ['bbox', 'both']:\n",
    "                bbox = ann['bbox']\n",
    "                rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], \n",
    "                                         linewidth=1, edgecolor=color, \n",
    "                                         facecolor='none')\n",
    "                #ax.add_patch(rect)\n",
    "            \n",
    "            # Display segmentation polygon\n",
    "            if display_type in ['seg', 'both']:\n",
    "                for seg in ann['segmentation']:\n",
    "                    poly = [(seg[i], seg[i+1]) for i in range(0, len(seg), 2)]\n",
    "                    polygon = patches.Polygon(poly, closed=True, edgecolor=color, \n",
    "                                              fill=False)\n",
    "                    ax.add_patch(polygon)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO annotations\n",
    "with open('COCO_output/train/coco_annotations.json', 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Get all image files\n",
    "image_dir = \"COCO_output/train/\"\n",
    "all_image_files = [os.path.join(image_dir, img['file_name']) for img in annotations['images']]\n",
    "\n",
    "#Get some image at random\n",
    "random_image_files = random.sample(all_image_files, 4)\n",
    "\n",
    "# Choose between 'bbox', 'seg', or 'both'\n",
    "display_type = 'seg'\n",
    "display_images_with_coco_annotations(random_image_files, annotations, display_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code transforms a dataset of images and annotations into a format suitable \n",
    "for training a YOLO (You Only Look Once) object detection model, and it also \n",
    "creates a YAML configuration file required for training the model.\n",
    "\n",
    "It reads coco style json annotations supplied as a single json file and also \n",
    "images as input. \n",
    "\n",
    "Here are the key steps in the code:\n",
    "\n",
    "1. Convert Images to YOLO Format: The convert_to_yolo function takes paths for \n",
    "input images and annotations (in JSON format), and directories to store the \n",
    "output images and labels. It then performs the following operations:\n",
    "\n",
    "- Reads the input JSON file containing annotations.\n",
    "- Copies all PNG images from the input directory to the output directory.\n",
    "- Normalizes the polygon segmentation data related to each image and writes \n",
    "them to text files, mapping them to the appropriate category .\n",
    "- The resulting text files contain information about the object category and the normalized coordinates of the polygons that describe the objects.\n",
    "\n",
    "2. Create YAML Configuration File: The create_yaml function takes paths to the input JSON file containing categories, training, validation, and optional test paths. It then:\n",
    "\n",
    "- Extracts the category names and the number of classes.\n",
    "- Constructs a dictionary containing information about class names, the number \n",
    "of classes, and paths to the training, validation, and test datasets.\n",
    "- Writes this dictionary to a YAML file, which can be used as a configuration \n",
    "file for training a model (e.g., a YOLO model).\n",
    "    \n",
    "\n",
    "\n",
    "The text annotation file consists of lines representing individual object \n",
    "annotations, with each line containing the class ID followed by the normalized \n",
    "coordinates of the polygon describing the object.\n",
    "\n",
    "Example structure of the YOLO annotation file:\n",
    "\n",
    "<class_id> <normalized_polygon_coordinate_1> <normalized_polygon_coordinate_2> ... <normalized_polygon_coordinate_n>\n",
    "0 0.123456 0.234567 0.345678 0.456789 ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to convert images to YOLO format\n",
    "def convert_to_yolo(input_images_path, input_json_path, output_images_path, output_labels_path):\n",
    "    # Open JSON file containing image annotations\n",
    "    f = open(input_json_path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # Create directories for output images and labels\n",
    "    os.makedirs(output_images_path, exist_ok=True)\n",
    "    os.makedirs(output_labels_path, exist_ok=True)\n",
    "\n",
    "    # List to store filenames\n",
    "    file_names = []\n",
    "    for filename in os.listdir(input_images_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            source = os.path.join(input_images_path, filename)\n",
    "            destination = os.path.join(output_images_path, filename)\n",
    "            shutil.copy(source, destination)\n",
    "            file_names.append(filename)\n",
    "\n",
    "    # Function to get image annotations\n",
    "    def get_img_ann(image_id):\n",
    "        return [ann for ann in data['annotations'] if ann['image_id'] == image_id]\n",
    "\n",
    "    # Function to get image data\n",
    "    def get_img(filename):\n",
    "        return next((img for img in data['images'] if img['file_name'] == filename), None)\n",
    "\n",
    "    # Iterate through filenames and process each image\n",
    "    for filename in file_names:\n",
    "        img = get_img(filename)\n",
    "        img_id = img['id']\n",
    "        img_w = img['width']\n",
    "        img_h = img['height']\n",
    "        img_ann = get_img_ann(img_id)\n",
    "\n",
    "        # Write normalized polygon data to a text file\n",
    "        if img_ann:\n",
    "            with open(os.path.join(output_labels_path, f\"{os.path.splitext(filename)[0]}.txt\"), \"a\") as file_object:\n",
    "                for ann in img_ann:\n",
    "                    current_category = ann['category_id'] - 1\n",
    "                    polygon = ann['segmentation'][0]\n",
    "                    normalized_polygon = [format(coord / img_w if i % 2 == 0 else coord / img_h, '.6f') for i, coord in enumerate(polygon)]\n",
    "                    file_object.write(f\"{current_category} \" + \" \".join(normalized_polygon) + \"\\n\")\n",
    "\n",
    "# Function to create a YAML file for the dataset\n",
    "def create_yaml(input_json_path, output_yaml_path, train_path, val_path, test_path=None):\n",
    "    with open(input_json_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract the category names\n",
    "    names = [category['name'] for category in data['categories']]\n",
    "    \n",
    "    # Number of classes\n",
    "    nc = len(names)\n",
    "\n",
    "    # Create a dictionary with the required content\n",
    "    yaml_data = {\n",
    "        'names': names,\n",
    "        'nc': nc,\n",
    "        'test': test_path if test_path else '',\n",
    "        'train': train_path,\n",
    "        'val': val_path\n",
    "    }\n",
    "\n",
    "    # Write the dictionary to a YAML file\n",
    "    with open(output_yaml_path, 'w') as file:\n",
    "        yaml.dump(yaml_data, file, default_flow_style=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input_path = \"COCO_output/\"\n",
    "base_output_path = \"yolo_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.join(base_input_path, \"val1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data.converter import convert_coco\n",
    "\n",
    "#Processing validation dataert\n",
    "#convert_coco(labels_dir=os.path.join(base_input_path, \"val\"),\n",
    " #            save_dir=os.path.join(base_output_path, \"valid_yolo\"),\n",
    "  #           use_keypoints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing validation dataset (if needed)\n",
    "convert_to_yolo(\n",
    "    input_images_path=os.path.join(base_input_path, \"val\"),\n",
    "    input_json_path=os.path.join(base_input_path, \"val/coco_annotations.json\"),\n",
    "    output_images_path=os.path.join(base_output_path, \"valid/images\"),\n",
    "    output_labels_path=os.path.join(base_output_path, \"valid/labels\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing training dataset \n",
    "convert_to_yolo(\n",
    "    input_images_path=os.path.join(base_input_path, \"train\"),\n",
    "    input_json_path=os.path.join(base_input_path, \"train/coco_annotations.json\"),\n",
    "    output_images_path=os.path.join(base_output_path, \"train/images\"),\n",
    "    output_labels_path=os.path.join(base_output_path, \"train/labels\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the YAML configuration file\n",
    "create_yaml(\n",
    "    input_json_path=os.path.join(base_input_path, \"train/coco_annotations.json\"),\n",
    "    output_yaml_path=os.path.join(base_output_path, \"data.yaml\"),\n",
    "    train_path=\"train/images\",\n",
    "    val_path=\"valid/images\",\n",
    "    test_path='../test/images'  # or None if not applicable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize YOLO labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_with_annotations(image_paths, annotation_paths):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    \n",
    "    for ax, img_path, ann_path in zip(axs.ravel(), image_paths, annotation_paths):\n",
    "        # Load image using OpenCV and convert it from BGR to RGB color space\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w, _ = image.shape\n",
    "        \n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')  # Turn off the axes\n",
    "\n",
    "        # Open the annotation file and process each line\n",
    "        with open(ann_path, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                category_id = int(parts[0])\n",
    "                color = tuple(np.random.rand(3))  # Generate a random RGB color\n",
    "                polygon = [float(coord) for coord in parts[1:]]\n",
    "                polygon = [coord * img_w if i % 2 == 0 else coord * img_h for i, coord in enumerate(polygon)]\n",
    "                polygon = [(polygon[i], polygon[i+1]) for i in range(0, len(polygon), 2)]\n",
    "                patch = patches.Polygon(polygon, closed=True, edgecolor=color, fill=False)\n",
    "                ax.add_patch(patch)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get all image files\n",
    "image_dir = \"yolo_dataset/train/images/\"\n",
    "annotation_dir = \"yolo_dataset/train/labels/\"\n",
    "all_image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "random_image_files = random.sample(all_image_files, 4)\n",
    "\n",
    "# Get corresponding annotation files\n",
    "image_paths = [os.path.join(image_dir, f) for f in random_image_files]\n",
    "annotation_paths = [os.path.join(annotation_dir, f.replace(\".png\", \".txt\")) for f in random_image_files]\n",
    "\n",
    "display_images_with_annotations(image_paths, annotation_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.patch_embed1.proj.weight', 'encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.weight', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.weight', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.center.top_conv.weight', 'decoder.center.top_conv.bias', 'decoder.center.center_conv.weight', 'decoder.center.center_conv.bias', 'decoder.center.bottom_conv.weight', 'decoder.center.bottom_conv.bias', 'decoder.center.out_conv.weight', 'decoder.center.out_conv.bias', 'decoder.blocks.0.hl_conv.0.0.weight', 'decoder.blocks.0.hl_conv.0.1.weight', 'decoder.blocks.0.hl_conv.0.1.bias', 'decoder.blocks.0.hl_conv.0.1.running_mean', 'decoder.blocks.0.hl_conv.0.1.running_var', 'decoder.blocks.0.hl_conv.0.1.num_batches_tracked', 'decoder.blocks.0.hl_conv.1.0.weight', 'decoder.blocks.0.hl_conv.1.1.weight', 'decoder.blocks.0.hl_conv.1.1.bias', 'decoder.blocks.0.hl_conv.1.1.running_mean', 'decoder.blocks.0.hl_conv.1.1.running_var', 'decoder.blocks.0.hl_conv.1.1.num_batches_tracked', 'decoder.blocks.0.SE_ll.1.weight', 'decoder.blocks.0.SE_ll.1.bias', 'decoder.blocks.0.SE_ll.3.weight', 'decoder.blocks.0.SE_ll.3.bias', 'decoder.blocks.0.SE_hl.1.weight', 'decoder.blocks.0.SE_hl.1.bias', 'decoder.blocks.0.SE_hl.3.weight', 'decoder.blocks.0.SE_hl.3.bias', 'decoder.blocks.0.conv1.0.weight', 'decoder.blocks.0.conv1.1.weight', 'decoder.blocks.0.conv1.1.bias', 'decoder.blocks.0.conv1.1.running_mean', 'decoder.blocks.0.conv1.1.running_var', 'decoder.blocks.0.conv1.1.num_batches_tracked', 'decoder.blocks.0.conv2.0.weight', 'decoder.blocks.0.conv2.1.weight', 'decoder.blocks.0.conv2.1.bias', 'decoder.blocks.0.conv2.1.running_mean', 'decoder.blocks.0.conv2.1.running_var', 'decoder.blocks.0.conv2.1.num_batches_tracked', 'decoder.blocks.1.hl_conv.0.0.weight', 'decoder.blocks.1.hl_conv.0.1.weight', 'decoder.blocks.1.hl_conv.0.1.bias', 'decoder.blocks.1.hl_conv.0.1.running_mean', 'decoder.blocks.1.hl_conv.0.1.running_var', 'decoder.blocks.1.hl_conv.0.1.num_batches_tracked', 'decoder.blocks.1.hl_conv.1.0.weight', 'decoder.blocks.1.hl_conv.1.1.weight', 'decoder.blocks.1.hl_conv.1.1.bias', 'decoder.blocks.1.hl_conv.1.1.running_mean', 'decoder.blocks.1.hl_conv.1.1.running_var', 'decoder.blocks.1.hl_conv.1.1.num_batches_tracked', 'decoder.blocks.1.SE_ll.1.weight', 'decoder.blocks.1.SE_ll.1.bias', 'decoder.blocks.1.SE_ll.3.weight', 'decoder.blocks.1.SE_ll.3.bias', 'decoder.blocks.1.SE_hl.1.weight', 'decoder.blocks.1.SE_hl.1.bias', 'decoder.blocks.1.SE_hl.3.weight', 'decoder.blocks.1.SE_hl.3.bias', 'decoder.blocks.1.conv1.0.weight', 'decoder.blocks.1.conv1.1.weight', 'decoder.blocks.1.conv1.1.bias', 'decoder.blocks.1.conv1.1.running_mean', 'decoder.blocks.1.conv1.1.running_var', 'decoder.blocks.1.conv1.1.num_batches_tracked', 'decoder.blocks.1.conv2.0.weight', 'decoder.blocks.1.conv2.1.weight', 'decoder.blocks.1.conv2.1.bias', 'decoder.blocks.1.conv2.1.running_mean', 'decoder.blocks.1.conv2.1.running_var', 'decoder.blocks.1.conv2.1.num_batches_tracked', 'decoder.blocks.2.hl_conv.0.0.weight', 'decoder.blocks.2.hl_conv.0.1.weight', 'decoder.blocks.2.hl_conv.0.1.bias', 'decoder.blocks.2.hl_conv.0.1.running_mean', 'decoder.blocks.2.hl_conv.0.1.running_var', 'decoder.blocks.2.hl_conv.0.1.num_batches_tracked', 'decoder.blocks.2.hl_conv.1.0.weight', 'decoder.blocks.2.hl_conv.1.1.weight', 'decoder.blocks.2.hl_conv.1.1.bias', 'decoder.blocks.2.hl_conv.1.1.running_mean', 'decoder.blocks.2.hl_conv.1.1.running_var', 'decoder.blocks.2.hl_conv.1.1.num_batches_tracked', 'decoder.blocks.2.SE_ll.1.weight', 'decoder.blocks.2.SE_ll.1.bias', 'decoder.blocks.2.SE_ll.3.weight', 'decoder.blocks.2.SE_ll.3.bias', 'decoder.blocks.2.SE_hl.1.weight', 'decoder.blocks.2.SE_hl.1.bias', 'decoder.blocks.2.SE_hl.3.weight', 'decoder.blocks.2.SE_hl.3.bias', 'decoder.blocks.2.conv1.0.weight', 'decoder.blocks.2.conv1.1.weight', 'decoder.blocks.2.conv1.1.bias', 'decoder.blocks.2.conv1.1.running_mean', 'decoder.blocks.2.conv1.1.running_var', 'decoder.blocks.2.conv1.1.num_batches_tracked', 'decoder.blocks.2.conv2.0.weight', 'decoder.blocks.2.conv2.1.weight', 'decoder.blocks.2.conv2.1.bias', 'decoder.blocks.2.conv2.1.running_mean', 'decoder.blocks.2.conv2.1.running_var', 'decoder.blocks.2.conv2.1.num_batches_tracked', 'decoder.blocks.3.conv1.0.weight', 'decoder.blocks.3.conv1.1.weight', 'decoder.blocks.3.conv1.1.bias', 'decoder.blocks.3.conv1.1.running_mean', 'decoder.blocks.3.conv1.1.running_var', 'decoder.blocks.3.conv1.1.num_batches_tracked', 'decoder.blocks.3.conv2.0.weight', 'decoder.blocks.3.conv2.1.weight', 'decoder.blocks.3.conv2.1.bias', 'decoder.blocks.3.conv2.1.running_mean', 'decoder.blocks.3.conv2.1.running_var', 'decoder.blocks.3.conv2.1.num_batches_tracked', 'decoder.blocks.4.conv1.0.weight', 'decoder.blocks.4.conv1.1.weight', 'decoder.blocks.4.conv1.1.bias', 'decoder.blocks.4.conv1.1.running_mean', 'decoder.blocks.4.conv1.1.running_var', 'decoder.blocks.4.conv1.1.num_batches_tracked', 'decoder.blocks.4.conv2.0.weight', 'decoder.blocks.4.conv2.1.weight', 'decoder.blocks.4.conv2.1.bias', 'decoder.blocks.4.conv2.1.running_mean', 'decoder.blocks.4.conv2.1.running_var', 'decoder.blocks.4.conv2.1.num_batches_tracked', 'segmentation_head.0.weight', 'segmentation_head.0.bias', 'cellprob_head.0.weight', 'cellprob_head.0.bias', 'cellprob_head.2.weight', 'cellprob_head.2.bias', 'cellprob_head.2.running_mean', 'cellprob_head.2.running_var', 'cellprob_head.2.num_batches_tracked', 'cellprob_head.3.weight', 'cellprob_head.3.bias', 'gradflow_head.0.weight', 'gradflow_head.0.bias', 'gradflow_head.2.weight', 'gradflow_head.2.bias', 'gradflow_head.2.running_mean', 'gradflow_head.2.running_var', 'gradflow_head.2.num_batches_tracked', 'gradflow_head.3.weight', 'gradflow_head.3.bias'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(\"from_phase2.pt\", map_location='cuda:0')\n",
    "print(state_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'best_fitness', 'model', 'ema', 'updates', 'optimizer', 'train_args', 'date', 'version'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"yolov8l-seg.pt\", map_location='cuda:0')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-11a1b009af50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Load the adapted weights into YOLOv8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov8l-seg.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"segment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adapted_weights.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"segment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/models/yolo/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Continue with default YOLO initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     def __call__(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_ckpt_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_safe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mDEFAULT_CFG_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_args\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# combine model and default args, preferring model args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ema\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0;31m# Model compatibility updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import collections\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model architecture\n",
    "model = YOLO(\"yolov8l-seg.yaml\", task=\"segment\")\n",
    "\n",
    "# Load your custom weights\n",
    "custom_weights = torch.load(\"from_phase2.pt\", map_location='cuda:0')\n",
    "\n",
    "# Convert OrderedDict to regular dict (if necessary)\n",
    "if isinstance(custom_weights, collections.OrderedDict):\n",
    "    custom_weights = dict(custom_weights)\n",
    "\n",
    "# Load the custom weights into the model\n",
    "model.model.load_state_dict(custom_weights, strict=False)\n",
    "\n",
    "# Create a new checkpoint dictionary with the expected structure\n",
    "new_checkpoint = {\n",
    "    'model': model.model.state_dict(),  # Save the model state dictionary\n",
    "    'epoch': 0,\n",
    "    'best_fitness': None,\n",
    "    'ema': None,\n",
    "    'updates': None,\n",
    "    'optimizer': None,\n",
    "    'train_args': {},\n",
    "    'date': None,\n",
    "    'version': None\n",
    "}\n",
    "\n",
    "# Save the new checkpoint\n",
    "torch.save(new_checkpoint, \"adapted_weights.pt\")\n",
    "\n",
    "# Load the adapted weights into YOLOv8\n",
    "model = YOLO(\"yolov8l-seg.yaml\", task=\"segment\")\n",
    "model = YOLO(\"adapted_weights.pt\", task=\"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-711daf1aba78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Load the adapted weights into YOLOv8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov8l-seg.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"segment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adapted_weights.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"segment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/models/yolo/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Continue with default YOLO initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     def __call__(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_ckpt_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_safe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mDEFAULT_CFG_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_args\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# combine model and default args, preferring model args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ema\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0;31m# Model compatibility updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import collections\n",
    "\n",
    "# Load your custom weights\n",
    "custom_weights = torch.load(\"from_phase2.pt\", map_location='cuda:0')\n",
    "\n",
    "# Convert OrderedDict to regular dict (if necessary)\n",
    "if isinstance(custom_weights, collections.OrderedDict):\n",
    "    custom_weights = dict(custom_weights)\n",
    "\n",
    "# Create a new dictionary with the expected structure\n",
    "new_checkpoint = {\n",
    "    'model': custom_weights,  # Assuming your custom weights contain the model parameters\n",
    "    'epoch': 0,\n",
    "    'best_fitness': None,\n",
    "    'ema': None,\n",
    "    'updates': None,\n",
    "    'optimizer': None,\n",
    "    'train_args': {},\n",
    "    'date': None,\n",
    "    'version': None\n",
    "}\n",
    "\n",
    "# Save the new checkpoint\n",
    "torch.save(new_checkpoint, \"adapted_weights.pt\")\n",
    "\n",
    "# Load the adapted weights into YOLOv8\n",
    "model = YOLO(\"yolov8l-seg.yaml\", task=\"segment\")\n",
    "model = YOLO(\"adapted_weights.pt\", task=\"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8l-seg.yaml\", task=\"segment\") # build a new model from yaml\n",
    "model = YOLO(\"yolo_dataset/results/125m_epochs/weights/best.pt\", task=\"segment\") # MEDIAR fintuned_hase2 pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yolo_dataset/data.yaml', 'r') as stream:\n",
    "    num_classes = str(yaml.safe_load(stream)['nc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 epochs training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellpose_projct = \"yolo_dataset/results\"\n",
    "name = \"125l_epochs_FocalLpsse\"\n",
    "#name = \"small_200_size640_epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters on COCO8 for 30 epochs\n",
    "#model.tune(data='yolo_dataset/data.yaml', epochs=100, iterations=300, optimizer='AdamW', plots=False, save=False, val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.86 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolo_dataset/results/125m_epochs/weights/best.pt, data=yolo_dataset/data.yaml, epochs=125, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=yolo_dataset/results, name=125l_epochs_FocalLpsse, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, loss=FocalLoss, tracker=botsort.yaml, save_dir=yolo_dataset/results/125l_epochs_FocalLpsse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 10:33:02,771\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==6.0.0 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-09-02 10:33:03,127\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==6.0.0 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   5159603  ultralytics.nn.modules.head.Segment          [1, 32, 192, [192, 384, 576]] \n",
      "YOLOv8m-seg summary: 331 layers, 27,240,227 parameters, 27,240,211 gradients, 110.4 GFLOPs\n",
      "\n",
      "Transferred 537/537 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolo_dataset/results/125l_epochs_FocalLpsse', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/train/labels.cache... 432 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 432/432 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to yolo_dataset/results/125l_epochs_FocalLpsse/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 86 weight(decay=0.0), 97 weight(decay=0.0005), 96 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolo_dataset/results/125l_epochs_FocalLpsse\u001b[0m\n",
      "Starting training for 125 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/125      26.1G      1.253      1.802     0.5553      1.052       2150        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:38<00:00,  1.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.771      0.831       0.51      0.869      0.752      0.815      0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/125      21.2G      1.246      1.789     0.5728      1.055       3580        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.771      0.826      0.503      0.863      0.759       0.81      0.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/125      27.7G      1.287      1.839     0.5889      1.061       3378        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.864      0.771      0.826      0.489      0.867      0.753      0.813      0.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/125      22.8G      1.292      1.851     0.6024      1.064       2976        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.857      0.745      0.805      0.484       0.85      0.736       0.79      0.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/125      25.6G      1.261      1.845     0.6069      1.064       2921        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.851       0.75      0.807      0.475      0.848      0.727      0.786      0.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/125      24.5G       1.32      1.917     0.6299      1.073       3250        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.859      0.754      0.807      0.475      0.846      0.737      0.789      0.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/125      23.7G      1.286       1.87     0.6156      1.073       3253        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.757      0.818      0.503      0.865      0.749      0.808       0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/125      18.9G      1.285      1.865     0.6166      1.079       3110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.857      0.767      0.822      0.505      0.856      0.761      0.814      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/125      22.6G      1.317      1.893     0.6175      1.078       4003        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.863      0.756      0.821      0.492      0.862      0.743      0.807      0.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     10/125      23.8G      1.329      1.876      0.622      1.074       4008        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.862      0.751      0.809      0.493       0.86      0.734      0.793      0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     11/125      26.3G      1.301      1.823     0.6072      1.069       2417        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878       0.75      0.822      0.504      0.877      0.736      0.809      0.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     12/125      25.5G      1.276      1.816     0.5991      1.062       3307        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.868       0.76      0.821      0.495      0.867      0.741      0.805      0.443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     13/125      24.4G      1.272      1.833     0.5997      1.068       2726        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.755      0.817      0.488      0.869       0.74      0.802      0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     14/125      22.3G      1.301      1.862     0.6035      1.071       1997        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.866      0.761      0.818      0.505      0.866      0.755      0.811      0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     15/125      20.7G      1.308      1.865     0.6066      1.069       2861        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.867      0.769      0.832      0.512      0.866      0.757      0.819      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     16/125      27.6G      1.294      1.852     0.5982      1.062       3090        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.754       0.82      0.491      0.856       0.74      0.802      0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     17/125      27.3G      1.252       1.79     0.5807      1.054       2332        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.865       0.75      0.815      0.484      0.863      0.728      0.798      0.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     18/125      26.9G      1.285      1.811     0.5976      1.058       2597        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.87      0.754      0.819      0.499      0.864      0.734      0.801      0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     19/125      19.4G      1.289      1.807     0.5923      1.061       2764        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.866       0.76      0.821      0.502      0.864      0.738      0.803      0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     20/125      18.3G      1.263      1.798     0.5854      1.063       2465        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.766      0.832      0.514      0.867      0.752      0.815      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     21/125      15.9G      1.261      1.801     0.5806      1.058       3462        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.758      0.817      0.497      0.864      0.742      0.799      0.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     22/125      21.4G      1.265      1.809     0.5825      1.056       3673        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.862      0.761      0.819      0.504      0.857      0.739      0.799      0.451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     23/125      23.4G      1.269      1.799     0.5846      1.057       3168        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.868      0.758      0.821      0.501      0.864      0.739      0.803       0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     24/125        24G      1.288      1.814     0.5887       1.07       3731        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.767      0.826      0.502      0.864      0.746      0.808      0.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     25/125      24.2G       1.26      1.791     0.5794      1.055       3592        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873       0.77      0.833      0.503      0.873      0.755      0.821      0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     26/125      21.3G      1.265      1.802     0.5816      1.052       3353        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.862       0.75      0.811      0.484      0.858      0.731      0.797      0.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     27/125      23.8G      1.251      1.776     0.5755      1.049       3321        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.862      0.753      0.814        0.5      0.856      0.736      0.798      0.448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     28/125      25.4G      1.243      1.768     0.5789      1.044       3157        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.858      0.768      0.819      0.499      0.855      0.756      0.808      0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     29/125      22.8G      1.259      1.773     0.5816      1.054       2814        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.867      0.759      0.824      0.497      0.866      0.742      0.807      0.451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     30/125      28.8G      1.257      1.794     0.5769      1.058       3980        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.756      0.825      0.511       0.87      0.746      0.814      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     31/125        21G      1.247       1.79     0.5703      1.053       3453        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.767      0.828      0.501      0.864      0.749      0.813      0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     32/125      20.6G      1.238      1.757     0.5663      1.048       3261        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.758      0.816      0.512      0.869      0.743      0.803      0.464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     33/125      24.9G      1.256      1.768     0.5755      1.052       2001        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.866       0.77      0.826      0.508      0.862      0.754      0.812       0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     34/125      26.4G      1.246      1.765     0.5693      1.055       3292        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.874      0.769      0.821      0.501      0.873      0.757       0.81      0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     35/125      22.3G      1.251      1.767     0.5621      1.042       3362        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.772      0.834      0.528      0.874      0.758      0.822      0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     36/125        25G      1.237      1.751     0.5595      1.046       3779        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.774       0.83       0.51      0.864      0.764      0.816       0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     37/125      23.9G      1.232       1.74     0.5569      1.041       3204        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.881      0.758      0.828      0.497      0.872      0.738      0.807      0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     38/125      26.9G      1.239      1.748     0.5579      1.041       2920        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.864      0.773      0.826      0.501      0.859      0.754      0.809      0.446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     39/125      19.8G      1.247      1.765     0.5692      1.048       3007        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.881      0.753       0.82      0.487      0.872      0.734        0.8      0.437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     40/125      26.8G      1.214      1.718     0.5559      1.038       3759        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.874      0.773      0.834      0.504      0.869      0.757      0.818      0.451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     41/125      24.4G      1.221      1.718     0.5549      1.046       2248        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.864      0.767      0.824      0.486      0.862      0.745      0.807      0.424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     42/125      24.9G       1.26      1.793     0.5687      1.042       3113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.879      0.765      0.824      0.504      0.877       0.75      0.814      0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     43/125      23.6G      1.224      1.736     0.5603       1.04       2611        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878      0.755      0.824      0.498      0.868      0.739      0.808      0.448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     44/125        22G      1.209       1.71     0.5492      1.035       2838        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.876      0.756      0.821        0.5      0.869      0.746       0.81      0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     45/125      25.5G      1.198       1.69     0.5417      1.035       2726        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.773      0.831       0.51      0.869       0.76      0.818      0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     46/125      23.2G      1.193      1.683     0.5427       1.03       2640        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.765      0.827      0.509       0.86      0.757      0.816      0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     47/125      23.6G      1.221      1.719     0.5504      1.031       2968        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.865      0.772      0.823      0.513      0.859      0.759       0.81      0.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     48/125      25.7G       1.23      1.741      0.559       1.05       2575        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.767      0.827      0.518      0.871      0.752      0.815      0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     49/125      19.8G      1.226      1.718     0.5484      1.034       3429        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.88      0.765      0.834      0.509      0.871      0.755      0.823      0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     50/125      21.7G      1.201      1.699     0.5395       1.03       3313        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.778      0.829      0.519      0.869       0.77      0.823      0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     51/125      24.9G      1.188      1.684     0.5355      1.023       3066        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.761      0.819      0.494      0.868      0.745      0.804      0.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     52/125      26.7G      1.197      1.683     0.5351      1.024       3001        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.867      0.766      0.827      0.513      0.871      0.751      0.815      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     53/125      21.1G      1.159      1.644     0.5252      1.025       2391        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.881      0.755      0.824      0.505      0.872      0.736      0.806      0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     54/125        26G      1.179      1.664     0.5308      1.022       2719        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.762       0.82      0.501      0.862      0.741      0.801      0.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     55/125      23.8G      1.218      1.719     0.5413      1.029       2792        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.876      0.772      0.826      0.522       0.88       0.76       0.82      0.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     56/125      26.5G      1.174       1.66     0.5291      1.028       2583        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.874      0.759      0.822      0.512      0.869      0.748      0.811      0.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     57/125      23.6G      1.204      1.687     0.5357      1.029       2404        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878      0.771      0.832      0.519      0.873      0.758       0.82      0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     58/125      21.4G      1.182      1.658     0.5267      1.019       2411        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873       0.77      0.827      0.512      0.868      0.756      0.814      0.464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     59/125      22.9G      1.176      1.663     0.5252      1.017       2112        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.776      0.828      0.521      0.869      0.762      0.818      0.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     60/125      21.8G      1.196      1.694     0.5339      1.027       3337        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.764      0.821      0.513      0.871      0.752       0.81      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     61/125      23.3G      1.181      1.675     0.5291      1.014       2442        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.866      0.776      0.829      0.507      0.866      0.758      0.818      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     62/125      23.8G      1.184      1.662     0.5307      1.019       3336        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.879      0.765      0.828      0.505      0.874      0.753      0.816      0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     63/125      24.2G      1.152      1.629     0.5176      1.014       2469        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.88      0.758      0.816      0.504      0.871      0.746      0.803      0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     64/125      24.5G      1.169      1.658     0.5214      1.015       3509        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.874       0.76      0.821      0.501      0.869      0.741      0.806      0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     65/125      26.3G      1.159      1.643     0.5162      1.015       2947        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.88      0.767      0.828      0.509      0.875      0.752      0.815      0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     66/125      23.3G      1.152      1.614     0.5116       1.01       3103        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.774      0.827      0.506      0.871      0.767      0.817      0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     67/125      21.3G      1.151      1.653     0.5124      1.009       3142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.777      0.834      0.512      0.863      0.763       0.82      0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     68/125      24.9G      1.173      1.669     0.5143      1.007       3526        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.776       0.83      0.505      0.869      0.761      0.816      0.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     69/125      20.3G      1.165      1.635     0.5134       1.02       3013        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878      0.774      0.833      0.518      0.874      0.762      0.822      0.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     70/125      18.1G      1.133      1.603     0.5007      1.007       3136        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.866       0.78       0.83      0.517      0.864      0.766      0.819      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     71/125        27G      1.156      1.629     0.5066      1.005       5120        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.773      0.828      0.515      0.872      0.755      0.817      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     72/125      27.6G      1.158      1.633     0.5082      1.005       3001        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.877      0.773      0.825      0.499      0.872      0.757      0.811      0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     73/125      24.1G       1.12      1.581     0.4999     0.9977       2922        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.769      0.825      0.508      0.878       0.75      0.814      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     74/125      20.9G      1.157      1.639     0.5126      1.008       2987        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.879      0.765      0.829      0.507      0.872       0.75      0.813      0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     75/125      20.5G       1.15      1.625     0.5095      1.006       3085        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.881      0.765      0.828      0.511      0.879       0.75      0.819      0.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     76/125      19.8G       1.16      1.643     0.5106      1.008       2496        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.776      0.829      0.498      0.868      0.757      0.815       0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     77/125      21.5G      1.139      1.626     0.5064      1.004       2596        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.772      0.824      0.502      0.865      0.759      0.809      0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     78/125      20.2G       1.13      1.612     0.4992      1.002       3196        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.774      0.828      0.511      0.873      0.755      0.815       0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     79/125      17.7G       1.12      1.588     0.4942      0.998       3070        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.769      0.822      0.503      0.871      0.751       0.81      0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     80/125      20.3G      1.122      1.578     0.4944     0.9923       2256        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.882      0.763       0.83      0.518      0.875      0.748      0.816      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     81/125      29.4G      1.144      1.612     0.5021     0.9974       3802        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.879      0.769       0.83      0.508      0.873      0.747      0.809      0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     82/125      25.2G      1.117      1.578     0.4919      1.001       2933        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.881      0.769      0.826      0.502      0.871      0.752       0.81      0.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     83/125      24.2G      1.135      1.599     0.4937     0.9987       3899        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878      0.771      0.831      0.504      0.871      0.758      0.818      0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     84/125      23.4G      1.119      1.575     0.4886     0.9974       2836        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.884      0.767      0.832      0.516      0.875      0.753       0.82       0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     85/125      27.1G      1.119      1.579     0.4899     0.9869       2514        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.774      0.831      0.522      0.866      0.758      0.818      0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     86/125      26.7G       1.14      1.623     0.5001      0.993       3931        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.876      0.768      0.832      0.517      0.873      0.746      0.816      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     87/125      19.1G      1.128      1.594     0.4949     0.9899       4013        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.879      0.772       0.83      0.522      0.874       0.76      0.819      0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     88/125      25.4G      1.114      1.567     0.4849     0.9883       4200        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.877      0.773      0.832      0.514      0.874      0.761      0.823      0.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     89/125      23.3G      1.104      1.577      0.483      0.994       2251        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.772      0.826      0.503      0.871      0.751       0.81      0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     90/125      27.5G      1.099      1.552     0.4828      0.987       3148        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.876       0.77      0.826      0.522      0.874      0.756      0.813      0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     91/125      23.3G      1.111      1.552     0.4815     0.9919       3359        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.772      0.825      0.512      0.873      0.758      0.813      0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     92/125      21.8G      1.095       1.54     0.4831     0.9875       2804        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.874      0.771      0.832      0.522      0.876      0.757      0.821      0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     93/125      24.8G      1.104       1.56      0.483     0.9882       2381        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878      0.776       0.83      0.507      0.874      0.759      0.815      0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     94/125      26.6G       1.11      1.565     0.4833     0.9791       3329        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.879      0.774      0.833      0.518      0.876      0.758       0.82      0.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     95/125      24.2G      1.129      1.595     0.4923     0.9897       3395        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.883      0.766      0.832      0.519      0.877      0.758      0.822      0.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     96/125      20.4G      1.102      1.555     0.4828     0.9792       2878        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878       0.77      0.828      0.512      0.875      0.758      0.816      0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     97/125      20.7G      1.094      1.549      0.481     0.9875       2243        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.771      0.827      0.514      0.869      0.761      0.817      0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     98/125      24.8G      1.084      1.528     0.4754     0.9862       2542        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.876      0.768      0.829      0.523       0.87      0.756      0.817      0.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     99/125        24G      1.113      1.571     0.4844     0.9762       2395        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.781      0.833      0.513      0.871       0.76      0.818      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    100/125      26.9G        1.1      1.557     0.4784     0.9769       4107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.883      0.765      0.832      0.517       0.88      0.752      0.819      0.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    101/125      25.6G      1.083      1.539     0.4755      0.979       3571        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.775      0.825      0.506      0.867       0.76      0.812      0.464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    102/125      31.4G      1.089      1.542     0.4737      0.975       3738        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.877      0.774      0.833      0.516      0.875       0.76      0.822      0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    103/125      21.8G      1.066      1.507     0.4646     0.9745       1958        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.885       0.77      0.832      0.518      0.883      0.757      0.822      0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    104/125      24.1G      1.081      1.539     0.4711     0.9792       1718        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.885      0.763      0.827      0.504      0.878      0.748      0.812      0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    105/125      22.6G      1.061      1.499     0.4617      0.975       3902        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.881      0.766      0.823      0.508      0.876      0.753      0.811      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    106/125      29.5G      1.064      1.514     0.4654     0.9764       3815        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.884      0.775      0.829      0.511      0.882      0.764      0.819      0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    107/125      23.1G      1.081      1.535     0.4698     0.9772       2254        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.88      0.772      0.829      0.515      0.879      0.761      0.818      0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    108/125        28G      1.069      1.518     0.4691     0.9726       2565        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.88      0.774      0.831      0.516      0.875      0.765       0.82      0.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    109/125      22.5G      1.047       1.49     0.4563     0.9693       3440        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878      0.776      0.832      0.521      0.872      0.765      0.822      0.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    110/125      16.5G      1.051      1.491     0.4595     0.9652       3018        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.876      0.776       0.83      0.519      0.875      0.766      0.823       0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    111/125      28.9G      1.051      1.491     0.4599     0.9676       2947        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.879      0.768       0.83      0.512      0.881      0.752       0.82      0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    112/125      21.7G      1.061      1.498     0.4606     0.9657       3033        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.774      0.831       0.52      0.876      0.757      0.819      0.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    113/125        25G      1.038      1.472     0.4549      0.964       2329        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.88      0.765      0.828      0.512      0.877       0.75      0.815       0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    114/125      21.5G       1.03      1.462     0.4506     0.9688       2222        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.883      0.766      0.827      0.519      0.879      0.755      0.816      0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    115/125      22.4G      1.042      1.474     0.4542     0.9683       3068        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:23<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.881       0.77       0.83      0.516      0.875      0.757      0.819      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    116/125      18.9G      1.067      1.527     0.4661     0.9928       2524        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:32<00:00,  1.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.879      0.775      0.823      0.512      0.868      0.766      0.811      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    117/125      19.7G      1.049      1.494      0.453     0.9869       1974        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.877      0.771      0.822      0.502      0.867      0.762      0.808      0.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    118/125      19.5G      1.055      1.493     0.4527     0.9796       1672        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.774      0.821      0.506      0.879      0.751      0.807      0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    119/125      23.4G       1.04      1.484     0.4477     0.9793       1530        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878      0.771      0.824       0.51      0.873      0.758      0.811      0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    120/125      18.5G       1.03      1.466      0.443      0.973       1812        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.879      0.768      0.824      0.513      0.882      0.752      0.813      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    121/125      19.1G      1.032      1.465     0.4416     0.9778        878        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.769      0.825      0.511      0.878      0.751      0.812      0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    122/125      19.1G      1.027      1.458     0.4401     0.9723       1728        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.771      0.826      0.513      0.876      0.754      0.813      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    123/125      20.1G      1.022      1.453     0.4375     0.9679       1576        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.877      0.772      0.827      0.514      0.877      0.757      0.815      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    124/125        20G      1.024      1.458     0.4357     0.9759       1678        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.878      0.772      0.828      0.514      0.872      0.761      0.817      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    125/125      18.1G      1.018       1.45     0.4331     0.9692       1860        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.775      0.828      0.514       0.87      0.762      0.817       0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "125 epochs completed in 0.987 hours.\n",
      "Optimizer stripped from yolo_dataset/results/125l_epochs_FocalLpsse/weights/last.pt, 54.8MB\n",
      "Optimizer stripped from yolo_dataset/results/125l_epochs_FocalLpsse/weights/best.pt, 54.8MB\n",
      "\n",
      "Validating yolo_dataset/results/125l_epochs_FocalLpsse/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:16<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.772      0.834      0.528      0.873      0.758      0.821      0.482\n",
      "Speed: 0.2ms preprocess, 10.4ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1myolo_dataset/results/125l_epochs_FocalLpsse\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "results = model.train(data='yolo_dataset/data.yaml',\n",
    "                    project=cellpose_projct,\n",
    "                    name=name,\n",
    "                    epochs=125\n",
    "                    )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = YOLO('yolo_dataset/results/125m_epochs/weights/best.pt')\n",
    "#metrics= model.val(data='yolo_dataset/data.yaml', device=0, plots=True)\n",
    "#metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#j_idx = metrics.matrix[0][0]/metrics.matrix.sum()\n",
    "#j_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data_dir, ious):\n",
    "\n",
    "    #ious = np.linspace(.5,1,50)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    jac_idx = []\n",
    "    for io in ious:\n",
    "        tp_eval = YOLO(model)\n",
    "        metrics = tp_eval.val(data= data_dir,\n",
    "                              iou=io,\n",
    "                              device=0\n",
    "                              )\n",
    "        precision.append(metrics.results_dict[\"metrics/precision(B)\"])\n",
    "        recall.append(metrics.results_dict[\"metrics/recall(B)\"])\n",
    "        conf_mtrx = metrics.confusion_matrix\n",
    "        j_idx = conf_mtrx.matrix[0][0]/conf_mtrx.matrix.sum()\n",
    "        jac_idx.append(j_idx)\n",
    "    return jac_idx, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/results/125l_epochs_FocalLpsse/weights/best.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:22<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.868      0.785      0.839      0.529      0.868      0.769      0.825      0.479\n",
      "Speed: 0.8ms preprocess, 42.6ms inference, 0.1ms loss, 4.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val357\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.868      0.785      0.839      0.529      0.868      0.769      0.825      0.479\n",
      "Speed: 0.3ms preprocess, 13.3ms inference, 0.0ms loss, 7.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val358\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.868      0.785      0.839      0.529      0.869      0.768      0.825      0.479\n",
      "Speed: 2.1ms preprocess, 13.9ms inference, 0.0ms loss, 11.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val359\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.868      0.785      0.839      0.529      0.868      0.769      0.825      0.479\n",
      "Speed: 2.2ms preprocess, 15.8ms inference, 0.0ms loss, 7.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val360\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.867      0.785      0.839      0.529      0.868      0.768      0.825      0.479\n",
      "Speed: 0.3ms preprocess, 13.6ms inference, 0.0ms loss, 14.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val361\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.782      0.838      0.529      0.868      0.768      0.825      0.478\n",
      "Speed: 0.2ms preprocess, 12.9ms inference, 0.0ms loss, 14.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val362\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872       0.78      0.838      0.529      0.868      0.768      0.825      0.478\n",
      "Speed: 0.3ms preprocess, 12.8ms inference, 0.0ms loss, 9.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val363\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:22<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.779      0.838      0.529      0.868      0.768      0.824      0.478\n",
      "Speed: 0.3ms preprocess, 12.1ms inference, 0.0ms loss, 8.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val364\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.779      0.838      0.529      0.867      0.768      0.824      0.478\n",
      "Speed: 0.3ms preprocess, 13.8ms inference, 0.0ms loss, 7.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val365\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872       0.78      0.838      0.529      0.869      0.765      0.824      0.478\n",
      "Speed: 0.5ms preprocess, 15.0ms inference, 0.0ms loss, 12.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val366\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871       0.78      0.837      0.529      0.869      0.765      0.824      0.478\n",
      "Speed: 0.7ms preprocess, 11.5ms inference, 0.0ms loss, 8.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val367\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:16<00:00, 19.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.87       0.78      0.837      0.529      0.868      0.765      0.824      0.478\n",
      "Speed: 0.3ms preprocess, 11.9ms inference, 0.0ms loss, 7.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val368\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.87      0.779      0.836      0.529      0.868      0.765      0.823      0.478\n",
      "Speed: 0.3ms preprocess, 14.4ms inference, 0.0ms loss, 4.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val369\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.779      0.836      0.529      0.868      0.765      0.823      0.478\n",
      "Speed: 0.5ms preprocess, 11.3ms inference, 0.1ms loss, 7.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val370\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.779      0.836      0.529      0.868      0.764      0.823      0.478\n",
      "Speed: 0.4ms preprocess, 11.7ms inference, 0.0ms loss, 3.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val371\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.777      0.836      0.529      0.871      0.761      0.822      0.477\n",
      "Speed: 0.3ms preprocess, 12.0ms inference, 0.0ms loss, 6.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val372\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.87      0.777      0.835      0.529      0.871      0.761      0.822      0.477\n",
      "Speed: 0.2ms preprocess, 13.7ms inference, 0.1ms loss, 12.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val373\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.87      0.777      0.835      0.528       0.87      0.761      0.822      0.477\n",
      "Speed: 0.2ms preprocess, 11.8ms inference, 0.0ms loss, 6.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val374\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.774      0.834      0.528       0.87       0.76      0.821      0.477\n",
      "Speed: 0.4ms preprocess, 20.4ms inference, 0.0ms loss, 9.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val375\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.87      0.774      0.834      0.528      0.869       0.76       0.82      0.477\n",
      "Speed: 1.1ms preprocess, 15.7ms inference, 0.0ms loss, 9.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val376\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.774      0.833      0.528      0.869      0.758       0.82      0.476\n",
      "Speed: 0.2ms preprocess, 18.2ms inference, 0.0ms loss, 4.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val377\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.772      0.832      0.527       0.87      0.757      0.819      0.476\n",
      "Speed: 0.2ms preprocess, 15.6ms inference, 0.0ms loss, 3.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val378\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.769      0.831      0.527       0.87      0.756      0.818      0.476\n",
      "Speed: 1.5ms preprocess, 22.6ms inference, 0.0ms loss, 5.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val379\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:03<00:00, 17.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.769      0.831      0.527      0.872      0.754      0.818      0.475\n",
      "Speed: 0.3ms preprocess, 11.1ms inference, 0.0ms loss, 6.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val380\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:22<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.871      0.766       0.83      0.527      0.871      0.754      0.817      0.475\n",
      "Speed: 0.2ms preprocess, 12.5ms inference, 0.0ms loss, 5.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val381\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.872      0.765      0.829      0.526       0.87      0.754      0.816      0.475\n",
      "Speed: 0.6ms preprocess, 11.4ms inference, 0.0ms loss, 11.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val382\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.874      0.761      0.828      0.526      0.871       0.75      0.815      0.474\n",
      "Speed: 0.2ms preprocess, 16.9ms inference, 0.0ms loss, 19.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val383\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.874      0.759      0.826      0.525      0.872      0.748      0.814      0.474\n",
      "Speed: 0.4ms preprocess, 14.0ms inference, 0.0ms loss, 6.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val384\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.756      0.824      0.525      0.872      0.746      0.813      0.474\n",
      "Speed: 2.3ms preprocess, 18.0ms inference, 0.0ms loss, 5.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val385\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.754      0.823      0.525      0.873      0.742      0.812      0.473\n",
      "Speed: 1.4ms preprocess, 16.3ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val386\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.877      0.749      0.822      0.524      0.872      0.741      0.811      0.473\n",
      "Speed: 0.3ms preprocess, 15.4ms inference, 0.0ms loss, 3.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val387\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.877      0.746       0.82      0.524      0.869      0.739      0.809      0.472\n",
      "Speed: 2.3ms preprocess, 12.2ms inference, 0.0ms loss, 8.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val388\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.875      0.744      0.817      0.523      0.867      0.736      0.806      0.472\n",
      "Speed: 0.2ms preprocess, 14.1ms inference, 0.0ms loss, 7.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val389\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:22<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.873      0.742      0.815      0.522      0.868      0.734      0.804      0.471\n",
      "Speed: 0.5ms preprocess, 11.4ms inference, 0.0ms loss, 10.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val390\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.87       0.74      0.811      0.521      0.865      0.731      0.802       0.47\n",
      "Speed: 0.5ms preprocess, 12.7ms inference, 0.0ms loss, 11.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val391\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.869      0.734      0.808      0.519      0.863      0.726      0.798      0.469\n",
      "Speed: 0.3ms preprocess, 15.3ms inference, 0.0ms loss, 9.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val392\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.864      0.729      0.804      0.518      0.859       0.72      0.794      0.467\n",
      "Speed: 0.2ms preprocess, 18.5ms inference, 0.0ms loss, 7.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val393\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:23<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.857      0.721      0.798      0.515      0.854      0.712      0.789      0.465\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 0.0ms loss, 25.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val394\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.843      0.716       0.79      0.512      0.841      0.708      0.782      0.462\n",
      "Speed: 0.2ms preprocess, 14.8ms inference, 0.0ms loss, 10.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val395\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.832      0.705      0.781      0.508      0.832      0.697      0.773      0.459\n",
      "Speed: 0.6ms preprocess, 13.2ms inference, 0.0ms loss, 15.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val396\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.817       0.69      0.766      0.501      0.817      0.684      0.759      0.453\n",
      "Speed: 0.2ms preprocess, 16.0ms inference, 0.0ms loss, 14.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val397\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:24<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.801      0.669      0.749      0.493      0.799      0.663      0.742      0.446\n",
      "Speed: 1.0ms preprocess, 10.4ms inference, 0.0ms loss, 10.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val398\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.77       0.65      0.725      0.481      0.772      0.642       0.72      0.437\n",
      "Speed: 0.3ms preprocess, 23.1ms inference, 0.1ms loss, 6.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val399\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.72      0.629      0.689      0.462      0.723      0.623      0.684      0.421\n",
      "Speed: 0.7ms preprocess, 10.1ms inference, 0.0ms loss, 10.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val400\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:22<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.653      0.597      0.632      0.432      0.658      0.591       0.63      0.394\n",
      "Speed: 0.2ms preprocess, 10.9ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val401\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091       0.56      0.552      0.542      0.379      0.561      0.553      0.541      0.348\n",
      "Speed: 0.2ms preprocess, 13.3ms inference, 0.0ms loss, 26.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val402\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.427      0.507      0.412      0.296      0.427      0.511      0.412      0.273\n",
      "Speed: 0.8ms preprocess, 11.6ms inference, 0.0ms loss, 18.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val403\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.292      0.457      0.268        0.2      0.291      0.461      0.268      0.184\n",
      "Speed: 0.2ms preprocess, 21.5ms inference, 0.1ms loss, 15.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val404\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:21<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.194      0.404      0.163      0.125      0.194      0.405      0.163      0.116\n",
      "Speed: 0.2ms preprocess, 11.5ms inference, 0.0ms loss, 10.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val405\u001b[0m\n",
      "Ultralytics YOLOv8.2.70 ðŸš€ Python-3.8.10 torch-2.2.1+cu121 CUDA:0 (Quadro RTX 8000, 48585MiB)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,222,963 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_data/yolo_dataset/valid/labels.cache... 108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        108      12091      0.176      0.389      0.141      0.109      0.176      0.389      0.141      0.101\n",
      "Speed: 0.2ms preprocess, 10.4ms inference, 0.0ms loss, 6.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/val406\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jac_idx, precision, recall = eval(model=\"yolo_dataset/results/125l_epochs_FocalLpsse/weights/best.pt\", data_dir=\"yolo_dataset/data.yaml\", ious=np.linspace(.5,1,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation medium model with best paramters from fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# DiffÃ©rents seuils de iou\n",
    "ious = np.linspace(.5,1,50)\n",
    "\n",
    "# DÃ©finir la figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Construction de la premiÃ¨re figure\n",
    "ax.plot(ious, precision, \"b\")\n",
    "ax.set_xlabel(\"Iou threshold\", fontsize= 14)\n",
    "ax.set_ylabel(\"Precision\", color=\"blue\", fontsize=14)\n",
    "ax.set_ylim([0, 1])\n",
    " \n",
    "# Construction de la seconde\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(ious, recall, \"r\")\n",
    "ax2.set_ylabel(\"Recall\", color =\"red\", fontsize=14)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "\n",
    "#Configuration des ligendes\n",
    "lines = [ax.get_lines()[0], ax2.get_lines()[0]]\n",
    "plt.legend(lines, [\"Precision\", \"recall\"], loc=\"lower left\")\n",
    "\n",
    "#Affichage du tout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Default parameters\n",
    "%matplotlib inline\n",
    "\n",
    "# DiffÃ©rents seuils de iou\n",
    "ious = np.linspace(.5,1,50)\n",
    "\n",
    "# DÃ©finir la figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Index de Jacard, precision, recall\n",
    "ax.plot(ious, jac_idx, color='green', label=\"Index de Jaccard\")\n",
    "ax.plot(ious, precision, color='blue', label=\"Precision\")\n",
    "ax.plot(ious, recall, color=\"red\", label=\"recall\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel('iou treshold')\n",
    "ax.set_ylabel(\"metriques\")\n",
    "ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAAsTAAALEwEAmpwYAAA58ElEQVR4nO3dd3hU1fbw8e/KpAOJEAJyAQGFS5MeQUTKT5SqKF5EsQEW9CpgbygKXrG8ouLFyrWDBcWGBbEAoqJ0CL0jBpASIEBISNvvH3uGTPokmZJk1ud59nPOzGnrpMyaffY5e4sxBqWUUsErJNABKKWUCixNBEopFeQ0ESilVJDTRKCUUkFOE4FSSgU5TQRKKRXkfJYIROQtEdkvImuLWC4i8l8R2SoiiSLS0VexKKWUKpovawTvAP2KWd4faOYso4BXfRiLUkqpIvgsERhjFgKHilnlUuA9Y/0BnCYi9XwVj1JKqcKFBvDY9YG/3F4nOd/bm39FERmFrTVQrVq1Ti1atPBLgEopVVUsX778oDEmvrBlgUwEHjPGTAOmASQkJJhly5YFOCKllKpcROTPopYF8q6h3UBDt9cNnO8ppZTyo0AmgtnA9c67h84FUowxBS4LKaWU8i2fXRoSkQ+BXkBtEUkCHgPCAIwxrwHfAgOArcAJYKSvYlFKKVU0nyUCY8ywEpYb4HZfHV8ppZRn9MlipZQKcpoIlFIqyGkiUEqpIKeJQCmlgpwmAqWUCnKaCJRSKshpIlBKqSCniUAppYKcJgKllApymgiUUirIaSJQSqkgp4lAKaWCnCYCpZQKcpVihDIVXDIy4NAhW5KTbTl0CFJSIC0NTpywU1fJyIDQUFvCwjybpqdDampuOX7cTnNyCu7LNZ+/hIdDSEjh+0lLg+hoiI3NLTExdlqtGkRGFiwOB2RlQXZ27jQ7GzIz7TmePGmLaz4ry8YWHm6LK6awMLs8PT3vzyktDYwBERu3q4hAzZrQogW0bAl169r3VPAImkSwYAF8803ef25XcThy//CNybtdeLj9J42KyjsNC7PLRfKWjAz7geVejh61Hw4hIbnHcz+2MfYDyJi885mZuR8CGRm58yK5//yelLCwvP/4rn/+kBC7PCKi4NThKLiua+pw5Bb314XNu7Zzcc1nZ8POnbBxoy2bNuXOHzxY/O8yJMT+HqKi7IdtWFjuB2ZWVuHTwojYD+Vq1aB6dTt1fRgXto/8JSfH7icsrOB+oqJs8lq3zv7+U1LsfgJNpODfeH6nnWYTQsuWNjmceWZuiY31S5jKz4ImEaxcCS+/XPwHg69ERtoPh5ycvN/2XFP3ROL64BTJ/YaX/9se5CYH93LypN1fZVSnjv3QGTwYGjaEuDhbatXKnY+Nzf3gL803Vldydf9Qd30LL8833+xsu1/X76SkGNLScms16em539hdU2NyvyS4EqrrC0NhyTo0NPd83L8sZGbm/QLjKhER9u/L/WfiKgcOwIYNecvXX8Nbb+U9j1q1bEJo2NAe+8SJ3BraiRP2XBo3hg4doH17O23VyrOfkQocMSV9PahgvDV4fU6O/UN2FXfutYOMjIL/sGlp9kPA9Q3eVcD+c7pfDoiNtf+U/uL6wMufJFy1DPd//pycvEnE/fKD6/xc67nPuy5ZuM+7v84/dXH/UxOBM86A5s1tqVXLfz8j5bkjR2DHDti+Pbfs2AF//WX/rqOj85bwcNiyBVavtokB7Htnn22TgitBtGtna0/Kf0RkuTEmodBlwZoIlFK+k50Nmzfbmrh7OXTILheBpk1tUmjTBpo1s6+bNrWXppT3aSJQSgWcMZCUBKtW5ZaVK20Nw11cnE0MLVvChRdCnz5Qu3YAAq5iNBEopSqsEyfsJaetW/OWVavsHWMi0Lkz9O9vS0JCbluH8pwmAqVUpZOdDcuXw5w5tixZYmsV8fFw++1wxx16Gak0iksEmleVUhWSw2FrAo89Bn/8Afv2wYwZ0KULTJhg706aMAEOHw5woFWAJgKlVKUQHw/XXANffQUrVsAFF8DEiTYhPPpobkO0Kj1NBEqpSqdDB/jsM9uOcNFF8J//2IRw5532IT5VOpoIlFKVVrt2MGsWJCbCxRfDK6/YZxbOOw/eeSf3WQZVPE0ESqlKr00b+OAD2L0bJk+2l4lGjoR69eC22+xDbqpomgiUUlVGfDzcc4/tImPhQrj0Unj7bXvL6bx5gY6u4tJEoJSqckSge3d47z37hPMZZ0C/frbWoArSRKCUqtIaNoRffrHtBtdcA88+W3IPrMFGE4FSqso77TSYOxeGDoX777cPo1XWnnp9IWi6oVZKBbeICPjwQ6hfH154AfbssQ+oRUYGOrLA00SglAoaISHw/PPQoIFtVD58GL7/3j7FHMz00pBSKujcfTe88Ya9k2jy5EBHE3iaCJRSQemGG+Bf/7LdUwT708iaCJRSQUnEPokcEwPDh/t/CNuKxKeJQET6icgmEdkqIg8WsvwMEZkvIitFJFFEBvgyHqWUclenDrz2mu3u+plnAh1N4PgsEYiIA3gZ6A+0AoaJSKt8qz0CfGyM6QBcBbziq3iUUqow//oXDBsGjz9ux1oORr6sEXQGthpjthtjMoCPgEvzrWOAGOd8LLDHh/EopVShpk61Q2QOHw4ZGYGOxv98mQjqA3+5vU5yvuduAnCtiCQB3wJjCtuRiIwSkWUisuzAgQO+iFUpFcTi4mDaNFsjeOKJQEfjf4FuLB4GvGOMaQAMAKaLSIGYjDHTjDEJxpiE+Ph4vweplKr6LrnE1giefNK2GQQTXyaC3UBDt9cNnO+5uxH4GMAY8zsQCdT2YUxKKVWkKVPg9NOD7xKRLxPBUqCZiDQRkXBsY/DsfOvsAnoDiEhLbCLQaz9KqYA47TR7F9G6dXZgm2Dhs0RgjMkCRgNzgQ3Yu4PWicjjIjLIudo9wM0ishr4EBhhjPYLqJQKnIEDoXNneOqp4Hm2QCrb525CQoJZtmxZoMNQSlVh33xjh7586y070llVICLLjTEJhS0LdGOxUkpVOAMGQMeOMGkSZGUFOhrf00SglFL5iMD48bBtG3z0UaCj8T1NBEopVYhBg6BtW/tcQVUfxEYTgVJKFSIkBB55BDZtglmzAh2Nb2kiUEqpIvzrX9CqFfznP5CTE+hofEcTgVJKFSEkBB5+2D5X8MUXgY7GdzQRKKVUMa68Ev75T9s7aSW7295jmgiUUqoYDoetFaxeDV99FehofEMTgVJKleDqq+HMM21bQVWsFWgiUEqpEoSGwrhxsGwZ/PRToKPxPk0ESinlgWuvhRo1YObMQEfifZoIlFLKAxERtkO6L7+seg+YBU8iyMqCtDQ4fhxSUuDQITh4EPbtg7177esTJ0r+DRtjuyTMzLTrVsULhkqpQl12GRw4AIsWBToS7woNdAB+8/zz8MADnq0bFgaRkfYrQE6O/dDPysqdFsbhsDcdh4ZCVJTdPjIydz4iInedkJC88yK2QO68iN1XaKiNx72EuOVv90Tkms8/LQ/3eFzFPXbXueQ/B/ep+7oiudvUqgX16tmRQE4/HerWteenVAXVvz+Eh8Pnn0P37oGOxnuCJxH07AlPP537IeQqrg+njAxITy9YQkIKfhCHhdltcnJsyc7Onc/MtNulpeXuIy0NTp7Mu47rtXutwpi8JTs7t/bhXvJ/wLs+cN3n80/LIn88kHsOhZ27a5v809IkpNq1balVC2rWLDgtbD4mJvd36UpKSvlATAxceKFNBM89V3X+1IInEXTpYosKDGMKJpDkZPj779yyd6+dHjwIhw/b1+vW2fmUlNIdL3+Nxf11SIitqVWvXrBUqwbR0QWLey3PvURF2RZEV4mOrjqfDqpQgwfDt9/a5wratw90NN4RPIlABZbrkpLDkftedDQ0bFj0Nu6ysuDIEZsUDh+2bTqu+aNHC9ZOCnvt/r6rvSg11U7374ft2207kXsprZAQmxCqV7fXEByO3Et8oaG5r101S/f58PDchOM+rV4d6tTJW+Lj7frK7wYNgltusbWCqpIIdIQypYpijL2050oK+S8bpqXZ948fh2PHbEI6diy3ZGXlluzsvK9dl/nc512XJ12XFdPSih9BPSbGFldtxpWAqleHuDibLFxJw1VOP90OzKu1lnLp0cN+L0lMDHQknituhDKtEShVFBH7jTwqyn6wBkJOjk0qBw7YWot7OXDALjt+PDcZ/fWXTUjJyUVfTouIsAnB1VBfr549v2rVCpaYGNtmEx9v22Pca3RBbPBguPtuO3DNWWcFOpry00SgVEUWEgKxsbY0bVq6bU+etO0tBw7kFtft0q72mC1b4Jdf7KW2kq4OiOStaTRrBi1a5JbGjYMmUbgSweefw733Bjqa8tNLQ0opmwTS0mybiXs5ejQ3ibgnlb17YfNm+55LeLhNDo0bQ/360KBB7rRBAzjjDHvZqoro0ME2c/32W6Aj8YxeGgJmrp3JtBXTEAQRIURCEJxTERziwBHiyDMNkZBTy13bndrG7bX71CEOQkNCCQ0JxRGSOx8aElrgGK73XMc4dTxy5wsr7vvNfzzX/hwhzqnb68L2W1g8hf0cRK8pV20iuXdIxcd7vl1ysh3Ca+PG3PLXX7BkiU0Y+cXH297b3EuzZtCmjW27qEQGD4YJE2zF6vTTAx1N+QRNIsg22WRkZ5BjcjDGYDCn5nNMDtkmm+yc7FNT13vu6+ffzv0919S1j6ycLLJysk69ruxcSaW4JOf+vvu6hZUwRxhhIWGEO8IJd4TnmY8IjSAyNJIIh526Sv73Xa/zrOO2TWhIqCYwX4uLg/POsyW/kydhzx5ISrLlzz9hxw57d9bixfDxx3mf5G/UCNq1swMFt2sHHTvaRFFBDR4Mjz1mu5y45ZZAR1M+emnID4wxNinkSzauhOFKRq4k4yqu991L/m2zc7JP7TsrJ8uuU0RCy7/fouLJ/5771P08TiU7kzcG9/UyszNPTV3rZ+ZknnqdkZ1RoJzMPklGdjF3y3goREKICo0iMjSSqLAookKjTk2jw6KpHl6dauHVqBZmS/Xw6lQPr06NiBrERMRQI9w5jahBjfAap9avHl6dcIfeulluWVm29rBxo70pf/VqexvOpk25CeKss2DAAFt69rQN9xWEMbYy07QpfPddoKMpWXGXhjQRqAopx+SQkZ1BelY6J7NOkp6Vbuez3eaLeN+9pGWmkZZli/vr1IxUUjNTC0yzcoroQiSfsJCwU4kjJiKm0BIbEctpkacVKHHRccRHxxMRGuHjn2IllZYG69fDH3/AnDkwb559LyoK/u//bD8PffvaT+AA1/juuw9efNFeBYuNDWgoJdJEoJQHjDGczD7JsZPHOJZxjKMnj3LspHOacYzUjFSOZxzPU45l5K7rXlLSU0jNTC32eDXCaxBfLZ746Hjiq8VTO7o2cVFx1IqqRa2oWqfm46vFc0bsGcRGxAbnpa60NPj5Z5sUvv0Wtm617zdqBBddZEvv3gG5xXfRIujWDd5/3w5eU5FpIlAqALJyskhJTyHlZApH0o9wJP0Ih9MOk5yWzIHUAxw44SypB9ifup/ktGQOpR3iRGbhTzTXCK/BGbFn5CkNYhrQMKYhDWIaUD+mPtFh0X4+ywDYuhV++MGWefPs8xIitk3hlltg5Ej7xLYf5OTYG6POPx8++cQvhywzTQRKVSLpWekcSjvEobRDJJ9IZl/qPv5K+YtdKbvYdXSXnabs4uCJgwW2jYuKo0FMA86seSbNajWjaa2mp0r9mPqESBXreT4rC5YutUnhyy9hxQpo2dJ2MHnJJX65dHTrrTBjhr08VIGaMArQRKBUFXQi8wS7j+4m6WgSfx39i6SjSSQdTWJXyi62H97OtsPb8jS6R4ZG0qJ2C9rUaUPbum1PlbrV6laNS07G2GTwwAP2GYfu3eHZZ33e2eTcudCvH8yebXNPRaWJQKkglJ2TTdLRJLYe2srWQ1vZcmgL6w6sY82+New+tvvUerWja9P+9PZ0/kdnOte3pV6NegGMvJwyM+HNN+29nfv3wxVX2FHnmzf3yeEyMuzjEVddBa+/7pNDeIUmAqVUHsknklmzfw1r9q0hcV8iK/5eQeK+xFN3TTWIaUDn+p3pUr8LPRr1oFO9ToQ5KtmgQceO2UEDJk+2nQNeeqm9zaewZx7KaeBA+4jE+vVe37XXaCJQSpUoLTONVX+vYvHuxSzZvYQlu5ew7fA2AKLDojmv4Xn0bNSTno160rl+58pz++u+ffDSS/DKK7ZPpa5dbQdBl17qtb6RnnkGHnzQHqpOHa/s0us0ESilymTf8X38susXft75Mwt3LSRxn+13OTI0kl6NezGg6QD6N+tP01ql7BAvEFJT4e237bC1O3bY5xAeeMDeZVTOhPD777ai8emncPnlXorXyzQRKKW84lDaIX7d9SvzdsxjztY5bE7eDECzWs3o37Q/A5oNoFfjXhW7tpCdbbsNffZZ2ydSQgK8+qqdllFGhu0qadQomDLFa5F6VcASgYj0A14EHMAbxpinC1lnKDABMMBqY0yxj2VoIlCq4th2aBtzts5hztY5zNsxj/SsdGqE12BAswFc1uIyBjQbQExETKDDLJwxMHMm3HWXvaZz223wxBNl7vyud287YN6KFd4N01sCkghExAFsBi4CkoClwDBjzHq3dZoBHwMXGGMOi0gdY8z+4variUCpiiktM435O+fzxcYv+HLTl+xP3U+4I5zeTXozuMVgLm95OXHRARrgpzgpKTB+PLz8sr3957nn7GPCpbylduJEWw4frpjdTRSXCEr9dImI1BSRth6s2hnYaozZbozJAD4CLs23zs3Ay8aYwwAlJQGlVMUVFRbFgGYDmHbJNPbcvYdfR/7KmM5j2HhwI6O+HkWDFxpw8+ybWbt/baBDzSs2Fv77X/tgWqNGcO21cOGFsHt3ydu66d7dVjIqy/gE7jxKBCKyQERiRKQWsAL4n4g8X8Jm9YG/3F4nOd9z90/gnyLym4j84byUVNjxR4nIMhFZdqCwPs6VUhWKI8RBtzO6MbnPZLaN3cbKW1ZyfdvrmbFmBm1ebcOF713IV5u+IsfkBDrUXB072s6DXn3VdpPdqZMdvc1D555re7ZYuNCHMfqIpzWCWGPMUeBy4D1jTBfgQi8cPxRoBvQChmETzGn5VzLGTDPGJBhjEuJLM2iGUirgRIT2p7fn9UteJ+muJJ7q/RSbkjcx6KNBNH+pOVMXT+V4xvFAh2k5HLbPiMWL7XjNF1wAU6eWPIwndkyfc84pVe6oMDxNBKEiUg8YCnzt4Ta7gYZurxs433OXBMw2xmQaY3Zg2xSaebh/pVQlExcdx4PnP8j2sduZOWQm8dHxjP1uLA1faMi4n8ax99jeQIdotW5t7yjq1w/GjoXhw20vqCXo0cNeYTpReL+BFZanieBxYC6wzRizVETOBLaUsM1SoJmINBGRcOAqYHa+db7A1gYQkdrYS0XbPYxJKVVJhTnCGNp6KItuXMSiGxbRu0lvnvntGRpNacTIL0dWjHaE006zfRdNmADTp9v+pnfuLHaTHj1sDxeLF/sjQO/xKBEYYz4xxrQ1xvzb+Xq7MeZfJWyTBYzGJpANwMfGmHUi8riIDHKuNhdIFpH1wHzgPmNMcllPRilV+XRt2JVZQ2exefRmbul0Cx+v+5g2r7ZhwPsD2HBgQ2CDCwmxfRZ99RVs22afNViypMjVu3WzNxtVtnYCj24fFZF/Aq8CdY0xZzvvGhpkjHnC1wHmp7ePKlW1JZ9I5rVlrzH598mkZqRyf7f7ebj7w0SFBbiP5y1b7MhoR4/Cr79CixaFrtaxI9SsCT/95Of4SuCN20f/BzwEZAIYYxKxl3qUUsqr4qLjeLjHw2y8fSNXnX0Vk36ZROtXWvPtlm8DG1izZvD997ZBuU8fSEoqdLUePWyXExnlH3bbbzxNBNHGmPz1Ic8Gd1VKqTKoW70u7w1+j/nD5xMRGsHADwYy5OMhJB0t/APYL1wj1R85YpNBcsEr2T162Hbl5cv9H15ZeZoIDorIWdhuIBCRIUAFad5XSlVlvRr3YvWtq5l0wSS+2fINLV9uyStLXwncMwgdOthRaLZtg4svtp3ZuTn/fDutTO0EniaC24HXgRYishu4E/i3r4JSSil34Y5wxnUfx/rb1nNew/O4/dvb6f1eb7YfDtBNhr16wYcf2objK66wtwo51aljmw8q0/MEnt41tN0YcyEQD7QwxpxvjNnp08iUUiqfJjWb8N013/HmoDdZsXcFbV5tw9TFUwNTO7j8cnjtNZgzB264wY5k79Sjh21Pzs72f1hl4WkXE4+KyKPAPcBdbq+VUsqvRIQbOtzAutvW0bNRT8Z+N5Ze7/Ri66Gt/g/m5pth0iQ7ev2kSafe7tHD9mW3Zo3/QyoLTy8NpbqVbKA/0NhHMSmlVIkaxDTgm6u/4e1L3yZxXyJtX23LjMQZ/g/koYfsgMVPPGFvMcUmAqg87QRl6oZaRCKAucaYXl6PqAT6HIFSKr/dR3dz7efXsmDnAl7q/xK3d77dvwHs3WsbBrp2tZeKRGjSxPZbN2uWf0Mpile7oXaKxvYdpJRSAVc/pj5zrpnDoOaDGD1nNE/98pR/A6hXDx5/HObOtaOfYWsFCxd61F9dwHnaRrBGRBKdZR2wCZji08iUUqoUIkMjmXXFLK5uczXj5o3jwR8fxK9D8d5+O7RtC3feCamp9OgBBw7Apk3+C6GsQj1c72K3+Sxgn7MvIaWUqjDCHGFMHzydmPAYnvntGVLSU3h54MuESFkvfpRCaCi88op9kOA//6H7DXZk3oULi+yNosLw9KdzzK2kATEiUstVfBadUkqVUoiE8MrAV7j/vPt5bflrXP/59WRmZ5a8oTd06wYjRsBzz9EsawN161aOBmNPawQrsGMLHAYEOA3Y5VxmgDO9HplSSpWRiPDMRc9wWuRpjJs3jvSsdGYOmYkjxOH7gz/zDHzxBTJmNN3O+5E//ijd2MeB4GmN4AfgEmNMbWNMHPZS0ffGmCbGGE0CSqkK6aHuD/Fcn+f4dMOn3Pr1rf5pM6hTB558EubN4/qImWzbZge0r8g8TQTnGmNOdf1njJkDnOebkJRSynvu7no3j3R/hDdWvsG4n8b556CjRkGnTvT7/m5qcJQVK/xz2LLyNBHsEZFHRKSxszwM7PFlYEop5S2P/9/j3NrpVp7+7WkmL5rs+wM6HPDKK4Qf/pvHmEhFf/TJ00QwDNvP0OfOUsf5nlJKVXgiwksDXmJo66Hc98N9vL3ybd8ftHNnZORIRvMS23/NP1x7xeJRY7Ex5hBwh49jUUopn3GEOJg+eDpH0o9w01c3UTOqJpe1uMy3B33kERxvv8d5C58Gpvr2WOVQbI1ARKY4p1+JyOz8xS8RKqWUl4Q7wvls6Gd0rt+Zq2Zdxfwd8317wCZNWJcwgquOTuPw2opbKyjp0tB053Qy8FwhRSmlKpVq4dX45upvaFqrKZfNvIxth7b59HjHx44jhByOPfKMT49THsUmAmPMchFxAKOMMT/nL36KUSmlvKpWVC2+vvprQiSEobOGkp6V7rNjtRrYhHcYwT++mQa7K2atoMTGYmNMNtBIRML9EI9SSvlF49Ma8+5l77Ji7wrumXuPz45Tsya8f8Y4O0rNMxWzVuDpXUPbgd9EZLyI3O0qvgxMKaV8bVDzQdzb9V5eWfYKM9fO9NlxTu/ahFnVhsO0ilkr8DQRbAO+dq5fw1mq+yoopZTylyd7P8l5Dc/jpq9uYnPyZp8co1MnGHd8HKaC1go8TQTrjTET3QuwwZeBKaWUP4Q5wvjoXx8R4Yjgik+uIC0zzevHSEiAHZzJ7t4Vs1bgaSJ4yMP3lFKq0mkY25Dpg6eTuC+RsXPGen3/HTva6eyzK2ZbQUnPEfQXkalAfRH5r1t5BzsugVJKVQn9m/Vn3PnjeGPlG0xfPb3kDUohNhaaNYMft58J119vawV7Kk4vPSXVCPYAy4B0YLlbmQ309W1oSinlXxP/byI9G/Xk1m9u9Xp7QUICLF8OPPwwZGVVqFpBSc8RrDbGvAs0BT4G/jDGvGuM+cwYU8E7VlVKqdIJDQnl/cvfJ8IRwYgvRpCdk+21fXfqBLt2wf7qZ8Lw4fD66/D3317bf3l42kbQD1gFfAcgIu21iwmlVFVUP6Y+U/tP5fek33n+9+e9tt+EBDtdvhx48EHIzIQpU7y2//LwNBFMADoDRwCMMauAJj6JSCmlAuzqNlczuMVgxs8fz/oD672yzw4d7HT5cmyDwZAhdozjI0e8sv/y8DQRZBpjUvK954ehfpRSyv9EhNcufo0aETUY/sVwsnLKf29MTAw0b07u2AQPPgjHjtlkEGCeJoJ1InI14BCRZs47iRb5MC6llAqoOtXq8OrAV1m2ZxlP//q0V/bZqZOzRgC2itC3r708lOb9ZxdKw9NEMAZoDZwEPgBS0PEJlFJV3JBWQxh29jAm/jyRVX+vKvf+EhIgKQn27XO+8dBDcOAAvPVWufddHp4mglbOEgpEApcCS30VlFJKVRQvDXiJ2tG1Gf7FcDKyM8q1r06d7PRUraBHD+jaFZ591jYeB4inieB94C3gcuBiZ7mkpI1EpJ+IbBKRrSLyYDHr/UtEjIgkeBiPUkr5Ra2oWvzvkv+RuC+Rx39+vFz76tABRNzaCURsreDPP+Gjj8ofbBl5mggOGGO+MsbsMMb86SrFbeAcx+BloD+2NjFMRFoVsl4N7GWmxaWMXSml/OLif17MiPYjePrXp1m+Z3nJGxShRg1o0cKtRgAwcCCcfTY8/TTk5JQ/2DLwNBE8JiJviMgwEbncVUrYpjOw1Riz3RiTAXyEvaSU33+AZ7BPLyulVIU0pe8UakXV4t4f7sWYst802amTW40AICTE3kG0fj189VX5Ay0DTxPBSKA99sGyS5zl4hK2qQ/85fY6yfneKSLSEWhojPmmuB2JyCgRWSYiyw4cOOBhyEop5T2xkbE81vMxFuxcwJytc8q8n4QE283Q3r1ub155JTRuDE89BeVIMmXlaSI4xxiTYIwZbowZ6Sw3lOfAIhICPA+UODSQMWaa8/gJ8fHx5TmsUkqV2ahOo2haqykP/PhAmbufKNBgDBAaCvfdB4sXw8/+HwXY00SwqLDr+yXYDTR0e93A+Z5LDeBsYIGI7ATOBWZrg7FSqqIKc4Tx5AVPsnb/Wt5b/V6Z9tG+vb0alOfyEMDIkVCnjq0V+JmnieBcYJXzDqBEEVkjIoklbLMUaCYiTZzjHV+F7bUUAGNMijGmtjGmsTGmMfAHMMgYk//Ho5RSFcaQVkPoXL8z4+ePL9MgNtWrF9JgDBAVBXfdBd9/b4sflabTuWZAH3LbB4q9fdQYkwWMBuZiRzP72BizTkQeF5FBZQ9ZKaUCR0R49qJn2X1sNy8ufrFM+0hIKKRGADB2LLRuDdde69fxCqQ8rd+BkJCQYJYV+hNUSin/ueTDS/jlz1/YNnYbcdFxpdp26lT7mZ+UBPXr51u4YQOcc44d1mzePNt+4AUistwYU+ild09rBEoppdw83ftpjmUcY9Ivk0q9ratL6kK/07ZsaUcw++UXGD++fEF6SBOBUkqVQes6rRnZfiQvLXmJHYd3lGrbdu3A4SgiEQBcfTWMGmUfMvum2LvrvUITgVJKldHEXhMJDQnlkfmPlGq76GjbFFDsVe4XX7S3GF13ne2Cwoc0ESilVBnVj6nPXefexQdrPmDF3hWl2vacc2wiKLKZNjISPvnEjm985ZWQUb4O74qjiUAppcrh/m73ExcVx7ifxpVqu4QEOHiwhC/7TZvaLqoXL4YHHihfoMXQRKCUUuUQGxnLA90eYO62ufy661ePtyu2wdjdkCH2FqMpU+Czz8ocZ3E0ESilVDnd3vl26lary/j5nt/l06YNhIV5kAjAjlcwdCiccUbZgyyGJgKllCqn6LBoxnUfx4KdC5i3Y55H20REQNu2HiaC8HCYOTO3GuFlmgiUUsoLRnUaRYOYBoyfP97jbqpLbDD2E00ESinlBZGhkTzc/WEW/bWI77Z+59E2CQmQkgJbt/o4uBJoIlBKKS+5ocMNND6tMY8ueNSjWoHHDcY+polAKaW8JNwRzqM9HmXZnmXM3jS7xPVbtbKPC2giUEqpKuS6dtfRrFYzxs8fT44pfgzisDD78LAmAqWUqkJCQ0KZ0GsCa/avYdb6WSWun5AAK1ZAdtkGPPMKTQRKKeVlV7a+klbxrXhswWMlDml5zjlw/Dhs3uyn4AqhiUAppbzMEeJgYq+JbDy4kQ/WfFDsuq4G46VL/RBYETQRKKWUD1ze8nLan96eJ355oti2gubNoVq1wLYTaCJQSikfCJEQHuj2AJuTN/P15q+LXM/hsIORaSJQSqkqaEirIZwRewaTF00udr2EBFi50vY4HQiaCJRSykdCQ0K569y7+GXXLyxOWlzkegkJkJ4O69f7MTg3mgiUUsqHbuxwI7ERsTz3+3NFrnPOOXYaqAZjTQRKKeVDNSJqcGvCrXy64dMixzY+6yyIjQ1cO4EmAqWU8rExncfgEAdT/phS6PKQEOjUSROBUkpVWfVj6jOszTDeXPkmh9IOFbpOQgKsXg0nT/o5ODQRKKWUX9zT9R5SM1N5fdnrhS5PSIDMTFi71s+BoYlAKaX8om3dtvQ5qw9Tl0zlZFbBr/2BfMJYE4FSSvnJvV3vZe/xvXy49sMCyxo3hri4wLQTaCJQSik/ufDMC2lbty2TF00uMHCNiK0VBCIRhPr/kN6XmZlJUlIS6enpgQ5FlUFkZCQNGjQgLCws0KEo5VMiwj1d72H4F8OZu20u/Zr2y7M8IQGefhrS0iAqyn9xVYlEkJSURI0aNWjcuDEiEuhwVCkYY0hOTiYpKYkmTZoEOhylfO6qs6/ioZ8eYvKiyQUSQdeudlyCjz6CkSP9F1OVuDSUnp5OXFycJoFKSESIi4vT2pwKGuGOcMZ2HstPO34icV9inmX9+0O3bnDPPfD33/6LqUokAkCTQCWmvzsVbG7udDNRoVFMXTw1z/shIfDmm3DiBIwe7b94qkwiUEqpyqJWVC2ua3sdM9bM4OCJg3mWNW8OEybAp5/a4g+aCLykevXqpVp/wYIFXHzxxV6Po3Hjxhw8eLDkFZ1uuukm1vuxy8MRI0Ywa1bJ47gqVdWN7TKW9Kx0/rf8fwWW3XMPdOgAt98Ohw/7PhafJgIR6Scim0Rkq4g8WMjyu0VkvYgkishPItLIl/Gogt544w1atWrlk31nBapzdaUqgdZ1WnPhmRfy8tKXyczOzLMsLMxeIjp40CYFX/PZXUMi4gBeBi4CkoClIjLbGOP+9XMlkGCMOSEi/wb+H3BleY5753d3survVeXZRQHtT2/PlH5TPFp3wYIFTJgwgdq1a7N27Vo6derEjBkzEBG+++477rzzTqKjozn//PNPbZOamsqYMWNYu3YtmZmZTJgwgUsvvZQ77riDuLg4Hn30UebOncukSZNYsGABISG5+Ts5OZlhw4axe/duunbtmufe5BkzZvDf//6XjIwMunTpwiuvvILD4cgTb69evZg8eTIJCQn8+9//ZunSpaSlpTFkyBAmTpwIwNKlS7njjjtITU0lIiKCn376ieTkZK677jpSU1MBeOmllzjvvPNYsGAB48ePp2bNmmzcuJFNmzYxZswYfvjhBxo2bEh4eHhZfw1KVTljO49l0EeD+Hzj5wxtPTTPsg4d4P774amn4KqroE8f38XhyxpBZ2CrMWa7MSYD+Ai41H0FY8x8Y8wJ58s/gAY+jMdvVq5cyZQpU1i/fj3bt2/nt99+Iz09nZtvvpmvvvqK5cuX87fbLQGTJk3iggsuYMmSJcyfP5/77ruP1NRUnnrqKWbOnMn8+fMZO3Ysb7/9dp4kADBx4kTOP/981q1bx+DBg9m1axcAGzZsYObMmfz222+sWrUKh8PB+++/X2zckyZNYtmyZSQmJvLzzz+TmJhIRkYGV155JS+++CKrV6/mxx9/JCoqijp16vDDDz+wYsUKZs6cydixY0/tZ8WKFbz44ots3ryZzz//nE2bNrF+/Xree+89Fi1a5MWftFKV28B/DuSsmmfx4uIXC13+6KO2zWDUKDh+3Hdx+PI5gvrAX26vk4Auxax/IzCnsAUiMgoYBXDGGWcUe1BPv7n7UufOnWnQwOa09u3bs3PnTqpXr06TJk1o1qwZANdeey3Tpk0D4Pvvv2f27NlMnmyHs0tPT2fXrl20bNmS//3vf/To0YMXXniBs846q8CxFi5cyGeffQbAwIEDqVmzJgA//fQTy5cv5xzniBdpaWnUqVOn2Lg//vhjpk2bRlZWFnv37mX9+vWICPXq1Tu1n5iYGMDWYkaPHn0qyWzevDnP+bueCVi4cCHDhg3D4XDwj3/8gwsuuKAMP1GlqqYQCWFM5zHcOfdOlu1ZRsI/EvIsj4y0l4i6d4eHH4YXC88X5VYhHigTkWuBBKBnYcuNMdOAaQAJCQmmsHUqkoiIiFPzDoejxGvlxhg+/fRTmjdvXmDZmjVriIuLY8+ePaWKwRjD8OHDeeqppzxaf8eOHUyePJmlS5dSs2ZNRowYUey9/S+88AJ169Zl9erV5OTkEBkZeWpZtWrVShWrUsFsZIeRPDL/EV5c/CLTB08vsLxbN9toPHUqDB1qX3ubLy8N7QYaur1u4HwvDxG5EHgYGGSMCUBP3P7RokULdu7cybZt2wD48MPcTqf69u3L1KlTT13fX7lyJQB//vknzz33HCtXrmTOnDksXlxwzNMePXrwwQcfADBnzhwOO28x6N27N7NmzWL//v0AHDp0iD///LPI+I4ePUq1atWIjY1l3759zJljK2fNmzdn7969LHV2iXjs2DGysrJISUmhXr16hISEMH36dLKzswvdb48ePZg5cybZ2dns3buX+fPne/5DUyoIxETEMLL9SGauncnfxwt/iuypp2ybQXKyb2LwZSJYCjQTkSYiEg5cBcx2X0FEOgCvY5PAfh/GEnCRkZFMmzaNgQMH0rFjxzyXacaPH09mZiZt27aldevWjB8/HmMMN954I5MnT+Yf//gHb775JjfddFOBb+mPPfYYCxcupHXr1nz22WenLp21atWKJ554gj59+tC2bVsuuugi9u7dW2hsIkK7du3o0KEDLVq04Oqrr6ab82tHeHg4M2fOZMyYMbRr146LLrqI9PR0brvtNt59913atWvHxo0bi6wFDB48mGbNmtGqVSuuv/56unbt6o0fp1JVypjOY8jKyeK1Za8Vurx6ddsZ3aBBvjm+5O8Bz6s7FxkATAEcwFvGmEki8jiwzBgzW0R+BNoArk+oXcaYYk81ISHBLMvXPd+GDRto2bKl1+MPBm3atGH27NkB7+dHf4cq2F38wcUs3bOUXXfuIiI0ouQNSklElhtjEgpb5tM2AmPMt8C3+d571G3+Ql8eXxXvoosuok2bNgFPAkopuKPLHfSZ0YeP133Mde2u8+uxK0RjsQqMH374IdAhKKWcLjzzQlrWbsmLi1/k2rbX+rUPLu1iQimlKgAR4Y4ud7B873Ke/OXJAgPX+JLWCJRSqoK4seON/PrXrzwy/xEOnjjIc32fI0R8/31dE4FSSlUQoSGhvHvZu8RFxTFl8RQOph3krUFvEebw7eh9mgiUUqoCCZEQXuj7AvHR8Twy/xEOpx3m4ys+Jjos2nfH9Nmeg4zD4aB9+/acffbZXHHFFZw4caLkjUrw6KOP8uOPPxa5/LXXXuO9994r93GUUhWLiPBwj4d5beBrfLvlW/pM78PhNN/1R+3T5wh8oaI+R1C9enWOO3uFuuaaa+jUqRN33333qeVZWVmEhmoFrCgV4XeoVEU0a/0srvnsGprHNWfutXOpV6NemfYTsOcIAuHOO2HVKu/us317mDLF8/W7d+9OYmJigS6ZN2zYwIMPPsiCBQs4efIkt99+O7fccgsAzzzzDDNmzCAkJIT+/fvz9NNPM2LECC6++GKGDBnCgw8+yOzZswkNDaVPnz5MnjyZCRMmUL16de69915WrVrFrbfeyokTJzjrrLN46623qFmzJr169aJLly7Mnz+fI0eO8Oabb9K9e3fv/oCUUj4zpNUQakbW5LKZl/H5xs+57ZzbvH6MKpcIAi0rK4s5c+bQr18/wHbJvHbtWpo0acK0adOIjY1l6dKlnDx5km7dutGnTx82btzIl19+yeLFi4mOjubQoUN59pmcnMznn3/Oxo0bERGOHDlS4LjXX389U6dOpWfPnjz66KNMnDiRKc7slZWVxZIlS/j222+ZOHFisZeblFIVT+8ze7P+tvU0iPFNT/1VLhGU5pu7N6WlpdG+fXvA1ghuvPFGFi1alKdL5u+//57ExMRTQzWmpKSwZcsWfvzxR0aOHEl0tG0MqlWrVp59x8bGEhkZyY033sjFF19cYIjLlJQUjhw5Qs+etvPW4cOHc8UVV5xafvnllwPQqVMndu7c6fVzV0r5XsPYhiWvVEZVLhEESlRUFKsKuSbl3hmbMYapU6fSt2/fPOvMnTu32H2HhoayZMkSfvrpJ2bNmsVLL73EvHnzPI7N1S22J11iK6WCj9415Ed9+/bl1VdfJTPTjk+6efNmUlNTueiii3j77bdP3WmU/9LQ8ePHSUlJYcCAAbzwwgusXr06z/LY2Fhq1qzJL7/8AsD06dNP1Q6UUqokWiPwo5tuuomdO3fSsWNHjDHEx8fzxRdf0K9fP1atWkVCQgLh4eEMGDCAJ5988tR2x44d49JLLyU9PR1jDM8//3yBfb/77runGovPPPNM3n77bX+emlKqEtPbR1WFoL9DpXyruNtH9dKQUkoFOU0ESikV5DQRKKVUkNNEoJRSQU4TgVJKBTlNBEopFeQ0EVRg77zzDqNHjwZgwoQJTJ48OcARKaWqIk0EPmCMIScnJ9BhKKWUR6rek8UB6od6586d9O3bly5durB8+XKGDh3K119/zcmTJxk8eDATJ04E4L333mPy5MmICG3btmX69Ol89dVXPPHEE2RkZBAXF8f7779P3bp1vXsOSilVhKqXCAJoy5YtvPvuuxw9epRZs2axZMkSjDEMGjSIhQsXEhcXxxNPPMGiRYuoXbv2qT6Fzj//fP744w9EhDfeeIP/9//+H88991yAz0YpFSyqXiIIVD/UQKNGjTj33HO59957+f777+nQoQNgO43bsmULq1ev5oorrqB27dpAbnfTSUlJXHnllezdu5eMjIxT3VYrpZQ/aBuBF7m6nDbG8NBDD7Fq1SpWrVrF1q1bufHGG4vcbsyYMYwePZo1a9bw+uuvk56e7q+QlVJKE4Ev9O3bl7feeuvUGMa7d+9m//79XHDBBXzyySckJycDud1Np6SkUL9+fcD2IqqUUv5U9S4NVQB9+vRhw4YNdO3aFbAD28+YMYPWrVvz8MMP07NnTxwOBx06dOCdd95hwoQJXHHFFdSsWZMLLriAHTt2BPgMlFLBRLuhVhWC/g6V8i3thloppVSRNBEopVSQqzKJoLJd4lK59HenVGBViUQQGRlJcnKyfqBUQsYYkpOTiYyMDHQoSgWtKnHXUIMGDUhKSuLAgQOBDkWVQWRkJA0aNAh0GEoFrSqRCMLCwvRpXKWUKiOfXhoSkX4isklEtorIg4UsjxCRmc7li0WksS/jUUopVZDPEoGIOICXgf5AK2CYiLTKt9qNwGFjTFPgBeAZX8WjlFKqcL6sEXQGthpjthtjMoCPgEvzrXMp4OpTYRbQW0TEhzEppZTKx5dtBPWBv9xeJwFdilrHGJMlIilAHHDQfSURGQWMcr48LiKbyhhT7fz7DgJ6zsFBzzk4lOecGxW1oFI0FhtjpgHTyrsfEVlW1CPWVZWec3DQcw4OvjpnX14a2g00dHvdwPleoeuISCgQCyT7MCallFL5+DIRLAWaiUgTEQkHrgJm51tnNjDcOT8EmGf0qTCllPIrn10acl7zHw3MBRzAW8aYdSLyOLDMGDMbeBOYLiJbgUPYZOFL5b68VAnpOQcHPefg4JNzrnTdUCullPKuKtHXkFJKqbLTRKCUUkGuSiYCD7q2GCEiB0RklbPcFIg4vamkc3auM1RE1ovIOhH5wN8xepsHv+cX3H7Hm0XkSADC9CoPzvkMEZkvIitFJFFEBgQiTm/x4HwbichPznNdICKVvvdCEXlLRPaLyNoilouI/Nf5M0kUkY7lPqgxpkoVbMP0NuBMIBxYDbTKt84I4KVAx+rnc24GrARqOl/XCXTcvj7nfOuPwd6wEPDYffx7ngb82znfCtgZ6Lh9fL6fAMOd8xcA0wMdtxfOuwfQEVhbxPIBwBxAgHOBxeU9ZlWsEXjStUVV48k53wy8bIw5DGCM2e/nGL2ttL/nYcCHfonMdzw5ZwPEOOdjgT1+jM/bPDnfVsA85/z8QpZXOsaYhdi7KItyKfCesf4AThOReuU5ZlVMBIV1bVG/kPX+5axWzRKRhoUsr0w8Oed/Av8Ukd9E5A8R6ee36HzD098zItIIaELuB0Zl5ck5TwCuFZEk4FtsTaiy8uR8VwOXO+cHAzVEJM4PsQWSx3/7nqqKicATXwGNjTFtgR/I7fiuKgvFXh7qhf12/D8ROS2QAfnRVcAsY0x2oAPxg2HAO8aYBthLCNNFpCr/n98L9BSRlUBPbG8FwfB79qqq+AdSYtcWxphkY8xJ58s3gE5+is1XPOnOIwmYbYzJNMbsADZjE0Nl5ck5u1xF5b8sBJ6d843AxwDGmN+BSGxHZZWRJ//Le4wxlxtjOgAPO9874rcIA6M0f/seqYqJoMSuLfJdTxsEbPBjfL7gSXceX2BrA4hIbeylou1+jNHbPDlnRKQFUBP43c/x+YIn57wL6A0gIi2xiaCyjuHqyf9ybbcaz0PAW36OMRBmA9c77x46F0gxxuwtzw4rRe+jpWE869pirIgMArKwjTIjAhawF3h4znOBPiKyHlt1vs8YU2k7+PPwnMF+eHxknLdbVGYenvM92Mt+d2EbjkdU1nP38Hx7AU+JiAEWArcHLGAvEZEPsedV29nW8xgQBmCMeQ3b9jMA2AqcAEaW+5iV9G9EKaWUl1TFS0NKKaVKQROBUkoFOU0ESikV5DQRKKVUkNNEoJRSQU4TgaqyRGSRl/YzQkT+4aV9vSMiQ0qxfuNieqFcICJBNXi78g1NBKrKMsac56VdjQAKTQQi4vDSMZQKGE0EqsoSkePOqYjIsyKyVkTWiMiVzvd7icjXbuu/JCIj8u1jCJAAvO8c1yBKRHaKyDMisgK4QkT6iMjvIrJCRD4RkerObZ8WO/5DoohMdtttDxFZJCLbXbWDomLMF0uUiHwkIhtE5HMgyrs/MRWsqtyTxUoV4nKgPdAO2+/OUhFZ6MmGxphZzqdb7zXGLAMQEYBkY0xHZ3cdnwEXGmNSReQB4G4ReRnbG2YLY4zJ18FfPeB8oAW2u4BZHsb4b+CEMaaliLQFVpTux6BU4bRGoILB+cCHxphsY8w+4GfgnHLuc6Zzei62T/zfRGQVMBxoBKQA6cCbInI5tisAly+MMTnGmPVA3VLE2AOYAWCMSQQSy3kOSgFaI1DBLYu8X4YiS7FtqnMqwA/GmGH5VxCRztgO4IYAo7EjaAGcdF+tFMdUyie0RqCCwS/AlSLiEJF47DfrJcCfQCsRiXBeuuldxPbHgBpFLPsD6CYiTQFEpJqI/NPZThBrjPkWuAt7yacsMbpbCFztPM7ZQNsS9qmUR7RGoILB50BX7GhWBrjfGPM3gIh8DKwFdmDHdC7MO8BrIpLm3M8pxpgDzgbmD0Ukwvn2I9jk8aWIRGK/9d9dlhhFpLHbOq8Cb4vIBmzX6ctL2KdSHtHeR5VSKsjppSGllApymgiUUirIaSJQSqkgp4lAKaWCnCYCpZQKcpoIlFIqyGkiUEqpIPf/AbVW86Oe9YBmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# DiffÃ©rents seuils de iou\n",
    "ious = np.linspace(.5,1,50)\n",
    "\n",
    "# DÃ©finir la figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Index de Jacard, precision, recall\n",
    "ax.plot(ious, jac_idx, color='green', label=\"Index de jacard\")\n",
    "ax.plot(ious, precision, color='blue', label=\"Precision\")\n",
    "ax.plot(ious, recall, color=\"red\", label=\"recall\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel('iou treshold')\n",
    "ax.set_ylabel(\"metriques\")\n",
    "ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation small model with best parameters from fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiffÃ©rents euils de iou\n",
    "ious = np.linspace(.5,1,50)\n",
    "\n",
    "# DÃ©finir la figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Construction de la premiÃ¨re figure\n",
    "ax.plot(ious, precision, \"b\")\n",
    "ax.set_xlabel(\"Iou threshold\", fontsize= 14)\n",
    "ax.set_ylabel(\"Precision\", color=\"blue\", fontsize=14)\n",
    " \n",
    "# Construction de la seconde\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(ious, recall, \"r\")\n",
    "ax2.set_ylabel(\"Recall\", color =\"red\", fontsize=14)\n",
    "\n",
    "#Configuration des ligendes\n",
    "lines = [ax.get_lines()[0], ax2.get_lines()[0]]\n",
    "plt.legend(lines, [\"Precision\", \"recall\"], loc=\"bottom left\")\n",
    "\n",
    "#Affichage du tout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_test/images/002_img.png: 480x640 102 Cellulars, 26.0ms\n",
      "Speed: 3.3ms preprocess, 26.0ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/segment/predict8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model1 = YOLO(\"yolo_dataset/results/125l_dflt_epochs/weights/best.pt\")\n",
    "\n",
    "image = \"/home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_test/images/002_img.png\"\n",
    "\n",
    "results = model1.predict(image, conf=0.5, show_labels=False, show_boxes=True, show=True, show_conf=False, save=True)  #Adjust conf threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_test/images/002_img.png: 480x640 102 Cellulars, 24.2ms\n",
      "Speed: 3.1ms preprocess, 24.2ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "model1 = YOLO(\"yolo_dataset/results/125l_dflt_epochs/weights/best.pt\")\n",
    "\n",
    "image = \"/home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_test/images/002_img.png\"\n",
    "\n",
    "#filename=\"yolo_dataset/results/125_epochs_seg_m_tune_best\"\n",
    "results = model1.predict(image, conf=0.5, show_labels=False, show_boxes=False, show_conf=False\n",
    "                          )  #Adjust conf threshold\n",
    "# Visualize the results\n",
    "for i, r in enumerate(results):\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot(labels=False, probs=False)  # BGR-order numpy array\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "\n",
    "    # Show results to screen (in supported environments)\n",
    "    r.show()\n",
    "\n",
    "    # Save results to disk\n",
    "    r.save(filename=f\"yolo_dataset/results/125l_dflt_epochs/002_img_seg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_masks(data_dir: str, image_dir: str, model: str):\n",
    "\n",
    "\n",
    "    # set the current directory to the image directory\n",
    "    #os.chdir(image_dir)\n",
    "\n",
    "    # create segmented masks directory\n",
    "    seg_masks_dir = os.path.join(data_dir, 'seg_masks')\n",
    "    if not os.path.exists(seg_masks_dir):\n",
    "        try:\n",
    "            os.makedirs(seg_masks_dir, exist_ok=True)\n",
    "        except os.error as e:\n",
    "            print(f'Same directory exist:{e}')\n",
    "            exit()\n",
    "    # set the path to the image directory\n",
    "    img_dir = os.path.join(data_dir, image_dir)\n",
    "\n",
    "    # iterate over all files in the test image directory\n",
    "    for file in os.listdir(img_dir):\n",
    "        # contruct full file path\n",
    "        file_path = os.path.join(img_dir, file)\n",
    "\n",
    "        # check if the file is image\n",
    "        if os.path.isfile(file_path) and file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp','.tif', '.tiff')):\n",
    "            # read the image with OpenCV\n",
    "            try:\n",
    "                img = cv2.imread(file_path)\n",
    "                h, w, _ = img.shape\n",
    "            except cv2.error as e:\n",
    "                print(f\"Failed to load image: {e}\")\n",
    "                exit()\n",
    "\n",
    "        # create the YOLOV8 \n",
    "        try:\n",
    "            mdel = YOLO(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model: {e}\")\n",
    "            exit()\n",
    "        \n",
    "        # Perform the prediction\n",
    "        img2 = cv2.resize(img, (640,640))\n",
    "        #img2 = cv2.resize(img, (img.shape[1], img.shape[0]))\n",
    "        #imgsz = (img2.shape[0], img2.shape[0])\n",
    "        results = mdel.predict(img2, iou=.5)\n",
    "\n",
    "        if(results[0].masks is not None):\n",
    "            # Get the size of the original image (height, width, channels)\n",
    "            h2, w2, c2 = results[0].orig_img.shape\n",
    "\n",
    "            # Create a black image with the same size as the original image\n",
    "            black_img = np.zeros_like(results[0].orig_img)\n",
    "            black_img = black_img[:,:,0]\n",
    "            \n",
    "            # Create a copy of the original image to layer the masks on\n",
    "            #layered_img = results[0].orig_img.copy()\n",
    "\n",
    "            # Loop over all masks in the results\n",
    "            for i, mask_raw in enumerate(results[0].masks):\n",
    "                # Convert mask to single channel image\n",
    "                mask_raw = mask_raw.cpu().data.numpy().transpose(1, 2, 0)\n",
    "\n",
    "                # Resize the mask to the same size as the image (can probably be removed if image is the same size as the model)\n",
    "                mask = cv2.resize(mask_raw, (w2, h2))\n",
    "\n",
    "                # Convert the mask to the correct data type\n",
    "                mask = mask.astype(np.int16)\n",
    "\n",
    "                #multiply by i+1\n",
    "                mask = mask*(i+1)\n",
    "\n",
    "                #add mask to black_img\n",
    "                black_img = np.maximum(black_img, mask)\n",
    "\n",
    "            # Break the loop if 'q' key is pressed\n",
    "            if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "        \n",
    "        name, ext = os.path.splitext(file)\n",
    "\n",
    "        # set the path to the segmented mask with the same naming system like images\n",
    "        seg_mask = os.path.join(seg_masks_dir, seg_masks_dir + '/' + name.replace(\"img\", \"masks\") + \"_seg.\" + ext)\n",
    "        \n",
    "        # Close all windows\n",
    "        black_img = cv2.resize(black_img, (w,h), interpolation=cv2.INTER_NEAREST)\n",
    "        cv2.imwrite(seg_mask, black_img)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/validation MIC-IPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 8 Cellulars, 36.5ms\n",
      "Speed: 3.4ms preprocess, 36.5ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 7 Cellulars, 32.8ms\n",
      "Speed: 2.3ms preprocess, 32.8ms inference, 16.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 17 Cellulars, 28.2ms\n",
      "Speed: 6.8ms preprocess, 28.2ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 17 Cellulars, 29.7ms\n",
      "Speed: 2.6ms preprocess, 29.7ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 12 Cellulars, 32.9ms\n",
      "Speed: 2.8ms preprocess, 32.9ms inference, 8.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 Cellulars, 29.7ms\n",
      "Speed: 3.6ms preprocess, 29.7ms inference, 7.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 17 Cellulars, 31.2ms\n",
      "Speed: 2.5ms preprocess, 31.2ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 7 Cellulars, 33.1ms\n",
      "Speed: 2.3ms preprocess, 33.1ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 13 Cellulars, 32.2ms\n",
      "Speed: 2.5ms preprocess, 32.2ms inference, 9.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 11 Cellulars, 26.9ms\n",
      "Speed: 1.8ms preprocess, 26.9ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 14 Cellulars, 36.9ms\n",
      "Speed: 3.0ms preprocess, 36.9ms inference, 7.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 12 Cellulars, 32.3ms\n",
      "Speed: 2.3ms preprocess, 32.3ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 23 Cellulars, 34.3ms\n",
      "Speed: 3.2ms preprocess, 34.3ms inference, 9.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 Cellulars, 32.8ms\n",
      "Speed: 2.5ms preprocess, 32.8ms inference, 6.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 7 Cellulars, 31.0ms\n",
      "Speed: 2.3ms preprocess, 31.0ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 12 Cellulars, 30.4ms\n",
      "Speed: 2.3ms preprocess, 30.4ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 9 Cellulars, 29.6ms\n",
      "Speed: 3.7ms preprocess, 29.6ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 10 Cellulars, 29.0ms\n",
      "Speed: 3.1ms preprocess, 29.0ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 Cellulars, 25.4ms\n",
      "Speed: 1.7ms preprocess, 25.4ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 11 Cellulars, 29.5ms\n",
      "Speed: 2.0ms preprocess, 29.5ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 Cellulars, 32.1ms\n",
      "Speed: 2.9ms preprocess, 32.1ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 16 Cellulars, 30.1ms\n",
      "Speed: 2.2ms preprocess, 30.1ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 12 Cellulars, 31.4ms\n",
      "Speed: 2.1ms preprocess, 31.4ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 13 Cellulars, 31.3ms\n",
      "Speed: 3.4ms preprocess, 31.3ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 21 Cellulars, 26.6ms\n",
      "Speed: 1.7ms preprocess, 26.6ms inference, 6.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 8 Cellulars, 36.3ms\n",
      "Speed: 2.4ms preprocess, 36.3ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 14 Cellulars, 35.9ms\n",
      "Speed: 2.6ms preprocess, 35.9ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 13 Cellulars, 31.9ms\n",
      "Speed: 2.9ms preprocess, 31.9ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 11 Cellulars, 29.1ms\n",
      "Speed: 2.5ms preprocess, 29.1ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "#HOME\n",
    "data_dir = \"/home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/validation MIC-IPA\"\n",
    "image_dir = \"cells/images\"\n",
    "model = \"yolo_dataset/results/125m_epochs/weights/best.pt\"\n",
    "seg_masks(data_dir=data_dir, image_dir=image_dir, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 103 Cellulars, 108.9ms\n",
      "Speed: 2.1ms preprocess, 108.9ms inference, 33.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "(640, 640)\n",
      "(640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Load the image\n",
    "try:\n",
    "    img = cv2.imread(\"/home/kamenan/Documents/Stage Dev Deep Learning/YOLOV8 evaluation/cellpose_test/images/052_img.png\")\n",
    "    h, w, c = img.shape\n",
    "except cv2.error as e:\n",
    "    print(f\"Failed to load image: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Create the YOLOv8 model\n",
    "try:\n",
    "    model = YOLO(\"yolo_dataset/results/125m_epochs/weights/best.pt\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Perform the prediction\n",
    "img2 = cv2.resize(img, (640,640))\n",
    "results = model.predict(img2, iou=.5)\n",
    "\n",
    "if(results[0].masks is not None):\n",
    "    # Get the size of the original image (height, width, channels)\n",
    "    h2, w2, c2 = results[0].orig_img.shape\n",
    "    print((w2, h2))\n",
    "\n",
    "    # Create a black image with the same size as the original image\n",
    "    black_img = np.zeros_like(results[0].orig_img)\n",
    "    black_img = black_img[:,:,0]\n",
    "    print(black_img.shape)\n",
    "    # Create a copy of the original image to layer the masks on\n",
    "    layered_img = results[0].orig_img.copy()\n",
    "\n",
    "    # Loop over all masks in the results\n",
    "    for i, mask_raw in enumerate(results[0].masks):\n",
    "        # Convert mask to single channel image\n",
    "        mask_raw = mask_raw.cpu().data.numpy().transpose(1, 2, 0)\n",
    "\n",
    "        # Resize the mask to the same size as the image (can probably be removed if image is the same size as the model)\n",
    "        mask = cv2.resize(mask_raw, (640, 640))\n",
    "\n",
    "        # Convert the mask to the correct data type\n",
    "        mask = mask.astype(np.uint8)\n",
    "        #print(mask.shape)\n",
    "\n",
    "        #multiply by i+1\n",
    "        mask = mask*(i+1)\n",
    "\n",
    "        #add mask to black_img\n",
    "        black_img = np.maximum(black_img, mask)\n",
    "        #black_img = black_img + mask\n",
    "        \n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        exit()\n",
    "\n",
    "# Close all windows\n",
    "#cv2.imwrite(\"mask_raw.png\", mask_raw)\n",
    "#cv2.imwrite(\"mask.png\", mask)\n",
    "black_img = cv2.resize(black_img, (w,h), interpolation=cv2.INTER_NEAREST)\n",
    "cv2.imwrite(\"black_img.png\", black_img)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 512, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
